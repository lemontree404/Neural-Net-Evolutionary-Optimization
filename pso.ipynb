{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data and Loading into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "# df1 = df[df[\"Personal Loan\"]==1]\n",
    "# df0 = df[df[\"Personal Loan\"]==0][:480]\n",
    "# df = pd.concat((df0,df1))\n",
    "# df = df.sample(frac=1)\n",
    "df.drop(\"ID\",axis=1,inplace=True)\n",
    "\n",
    "features = df.drop(\"Personal Loan\",axis=1).columns\n",
    "target = \"Personal Loan\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(df[features],df[target],test_size=0.2)\n",
    "\n",
    "traindb = pd.concat((trainx,trainy),axis=1)\n",
    "testdb = pd.concat((testx,testy),axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "class db(Dataset):\n",
    "    def __init__(self,target,features,df) -> None:\n",
    "        self.y  = torch.tensor(df[target].values)\n",
    "        self.x = torch.tensor(df[features].values)\n",
    "        self.x = scaler.fit_transform(self.x)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "\n",
    "traindbt = db(target,features,traindb)\n",
    "testdbt = db(target,features,testdb)\n",
    "\n",
    "train_loader = DataLoader(traindbt,batch_size=len(traindb),shuffle=True)\n",
    "test_loader = DataLoader(testdbt,batch_size=len(testdb),shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Particle Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = 10\n",
    "\n",
    "def fitness(position,model,loss,x,y):\n",
    "    a = model.l1.in_features * model.l1.out_features\n",
    "    b = a + model.l1.out_features\n",
    "    c = b + (model.l3.in_features * model.l3.out_features)\n",
    "    # d = c + model.l3.out_features\n",
    "\n",
    "    weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "    bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "    weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "    bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "    model.l1.weight = nn.Parameter(weights1)\n",
    "    model.l1.bias = nn.Parameter(bias1)\n",
    "    model.l3.weight = nn.Parameter(weights2)\n",
    "    model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "    y_ = model(x)\n",
    "    j = loss(y_,y.unsqueeze(-1))\n",
    "    \n",
    "    return j\n",
    "\n",
    "class particle:\n",
    "    def __init__(self,w,c1,c2,l,model,loss,x,y):\n",
    "        self.l = l\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.x, self.y = x,y\n",
    "        self.position = np.random.random(l)*20-xlim\n",
    "        self.velocity = np.random.random(l)\n",
    "        self.fitness = fitness(self.position,self.model,self.loss,self.x,self.y)\n",
    "        self.pbest = [self.fitness] + [i for i in self.position]\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.r1, self.r2 = np.random.random(2)\n",
    "\n",
    "    def calc_velocity(self,gbest):\n",
    "        soc = self.calc_soc(gbest)\n",
    "        cog = self.calc_cog()\n",
    "\n",
    "        self.velocity = self.w * self.velocity + soc + cog\n",
    "    \n",
    "    def calc_soc(self,gbest):\n",
    "        return self.c1 * self.r1 * (np.array([i for i in gbest][1:]) - self.position)\n",
    "    \n",
    "    def calc_cog(self):\n",
    "        return self.c2 * self.r2 * (np.array([i for i in self.pbest][1:]) - self.position)\n",
    "    \n",
    "    def iterate(self,gbest):\n",
    "\n",
    "        self.calc_velocity(gbest)\n",
    "\n",
    "        self.position += self.velocity\n",
    "\n",
    "        self.position = np.clip(self.position,-xlim,xlim)\n",
    "\n",
    "        self.fitness = fitness(self.position,self.model,self.loss,self.x,self.y)\n",
    "\n",
    "        if self.fitness<self.pbest[0]:\n",
    "            self.pbest[0] = self.fitness\n",
    "            for i in range(len(self.position)):\n",
    "                self.pbest[i+1] = self.position[i]\n",
    "        \n",
    "        self.r1, self.r2 = np.random.rand(2)*2\n",
    "\n",
    "def calc_gbest(l):\n",
    "    global gbest\n",
    "    for i in l:\n",
    "        if i.pbest[0]<gbest[0]:\n",
    "            gbest[0] = i.pbest[0]\n",
    "            for j in range(len(i.position)):\n",
    "                gbest[j+1] = i.position[j]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network (PyTorch)\n",
    "### Neural Network Architecture:\n",
    "- Input Layer: 12\n",
    "- Hidden Layer: 16\n",
    "- Output Layer: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(net,self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(12,16)\n",
    "        self.l2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(16,1)\n",
    "        self.l4 = nn.Sigmoid()\n",
    "        # self.l5 = nn.Linear(20,1)\n",
    "        # self.l6 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        x = X\n",
    "\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        # x = self.l5(x)\n",
    "        # x = self.l6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Step: 0 Loss: 9.311022758483887\n",
      "Epoch: 1/1000 Step: 0 Loss: 8.740114212036133\n",
      "Epoch: 2/1000 Step: 0 Loss: 6.079076766967773\n",
      "Epoch: 3/1000 Step: 0 Loss: 5.8807692527771\n",
      "Epoch: 4/1000 Step: 0 Loss: 5.250324249267578\n",
      "Epoch: 5/1000 Step: 0 Loss: 4.473321914672852\n",
      "Epoch: 6/1000 Step: 0 Loss: 3.2681779861450195\n",
      "Epoch: 7/1000 Step: 0 Loss: 2.279283285140991\n",
      "Epoch: 8/1000 Step: 0 Loss: 2.0014750957489014\n",
      "Epoch: 9/1000 Step: 0 Loss: 2.0014750957489014\n",
      "Epoch: 10/1000 Step: 0 Loss: 1.749413013458252\n",
      "Epoch: 11/1000 Step: 0 Loss: 1.749413013458252\n",
      "Epoch: 12/1000 Step: 0 Loss: 1.597507357597351\n",
      "Epoch: 13/1000 Step: 0 Loss: 1.5912160873413086\n",
      "Epoch: 14/1000 Step: 0 Loss: 1.4427019357681274\n",
      "Epoch: 15/1000 Step: 0 Loss: 1.3049299716949463\n",
      "Epoch: 16/1000 Step: 0 Loss: 1.1568392515182495\n",
      "Epoch: 17/1000 Step: 0 Loss: 1.0250221490859985\n",
      "Epoch: 18/1000 Step: 0 Loss: 0.8341812491416931\n",
      "Epoch: 19/1000 Step: 0 Loss: 0.6874483227729797\n",
      "Epoch: 20/1000 Step: 0 Loss: 0.5920668840408325\n",
      "Epoch: 21/1000 Step: 0 Loss: 0.589150071144104\n",
      "Epoch: 22/1000 Step: 0 Loss: 0.5365539193153381\n",
      "Epoch: 23/1000 Step: 0 Loss: 0.5148276686668396\n",
      "Epoch: 24/1000 Step: 0 Loss: 0.5130252838134766\n",
      "Epoch: 25/1000 Step: 0 Loss: 0.48190733790397644\n",
      "Epoch: 26/1000 Step: 0 Loss: 0.47371384501457214\n",
      "Epoch: 27/1000 Step: 0 Loss: 0.4561314880847931\n",
      "Epoch: 28/1000 Step: 0 Loss: 0.442847341299057\n",
      "Epoch: 29/1000 Step: 0 Loss: 0.40049663186073303\n",
      "Epoch: 30/1000 Step: 0 Loss: 0.39406201243400574\n",
      "Epoch: 31/1000 Step: 0 Loss: 0.3724599778652191\n",
      "Epoch: 32/1000 Step: 0 Loss: 0.36423957347869873\n",
      "Epoch: 33/1000 Step: 0 Loss: 0.34183207154273987\n",
      "Epoch: 34/1000 Step: 0 Loss: 0.3287639915943146\n",
      "Epoch: 35/1000 Step: 0 Loss: 0.3092225193977356\n",
      "Epoch: 36/1000 Step: 0 Loss: 0.28728652000427246\n",
      "Epoch: 37/1000 Step: 0 Loss: 0.27356794476509094\n",
      "Epoch: 38/1000 Step: 0 Loss: 0.260978639125824\n",
      "Epoch: 39/1000 Step: 0 Loss: 0.2563632130622864\n",
      "Epoch: 40/1000 Step: 0 Loss: 0.25275662541389465\n",
      "Epoch: 41/1000 Step: 0 Loss: 0.25078073143959045\n",
      "Epoch: 42/1000 Step: 0 Loss: 0.246029332280159\n",
      "Epoch: 43/1000 Step: 0 Loss: 0.24355508387088776\n",
      "Epoch: 44/1000 Step: 0 Loss: 0.2394873946905136\n",
      "Epoch: 45/1000 Step: 0 Loss: 0.23847079277038574\n",
      "Epoch: 46/1000 Step: 0 Loss: 0.235630065202713\n",
      "Epoch: 47/1000 Step: 0 Loss: 0.2311810553073883\n",
      "Epoch: 48/1000 Step: 0 Loss: 0.22841297090053558\n",
      "Epoch: 49/1000 Step: 0 Loss: 0.22518454492092133\n",
      "Epoch: 50/1000 Step: 0 Loss: 0.22361163794994354\n",
      "Epoch: 51/1000 Step: 0 Loss: 0.22188174724578857\n",
      "Epoch: 52/1000 Step: 0 Loss: 0.2206425815820694\n",
      "Epoch: 53/1000 Step: 0 Loss: 0.2195822149515152\n",
      "Epoch: 54/1000 Step: 0 Loss: 0.2183770388364792\n",
      "Epoch: 55/1000 Step: 0 Loss: 0.2159961760044098\n",
      "Epoch: 56/1000 Step: 0 Loss: 0.21476303040981293\n",
      "Epoch: 57/1000 Step: 0 Loss: 0.2131241112947464\n",
      "Epoch: 58/1000 Step: 0 Loss: 0.2119145691394806\n",
      "Epoch: 59/1000 Step: 0 Loss: 0.21081624925136566\n",
      "Epoch: 60/1000 Step: 0 Loss: 0.20989221334457397\n",
      "Epoch: 61/1000 Step: 0 Loss: 0.2082226276397705\n",
      "Epoch: 62/1000 Step: 0 Loss: 0.20672135055065155\n",
      "Epoch: 63/1000 Step: 0 Loss: 0.20440949499607086\n",
      "Epoch: 64/1000 Step: 0 Loss: 0.20283368229866028\n",
      "Epoch: 65/1000 Step: 0 Loss: 0.20088930428028107\n",
      "Epoch: 66/1000 Step: 0 Loss: 0.19881229102611542\n",
      "Epoch: 67/1000 Step: 0 Loss: 0.19694317877292633\n",
      "Epoch: 68/1000 Step: 0 Loss: 0.1942024677991867\n",
      "Epoch: 69/1000 Step: 0 Loss: 0.19288109242916107\n",
      "Epoch: 70/1000 Step: 0 Loss: 0.19079992175102234\n",
      "Epoch: 71/1000 Step: 0 Loss: 0.18942305445671082\n",
      "Epoch: 72/1000 Step: 0 Loss: 0.18825335800647736\n",
      "Epoch: 73/1000 Step: 0 Loss: 0.18711280822753906\n",
      "Epoch: 74/1000 Step: 0 Loss: 0.18486498296260834\n",
      "Epoch: 75/1000 Step: 0 Loss: 0.18339455127716064\n",
      "Epoch: 76/1000 Step: 0 Loss: 0.18167324364185333\n",
      "Epoch: 77/1000 Step: 0 Loss: 0.1802198439836502\n",
      "Epoch: 78/1000 Step: 0 Loss: 0.17874746024608612\n",
      "Epoch: 79/1000 Step: 0 Loss: 0.17796975374221802\n",
      "Epoch: 80/1000 Step: 0 Loss: 0.17747637629508972\n",
      "Epoch: 81/1000 Step: 0 Loss: 0.17708416283130646\n",
      "Epoch: 82/1000 Step: 0 Loss: 0.17673863470554352\n",
      "Epoch: 83/1000 Step: 0 Loss: 0.17640267312526703\n",
      "Epoch: 84/1000 Step: 0 Loss: 0.1758754402399063\n",
      "Epoch: 85/1000 Step: 0 Loss: 0.1755489856004715\n",
      "Epoch: 86/1000 Step: 0 Loss: 0.17516359686851501\n",
      "Epoch: 87/1000 Step: 0 Loss: 0.17485807836055756\n",
      "Epoch: 88/1000 Step: 0 Loss: 0.17448721826076508\n",
      "Epoch: 89/1000 Step: 0 Loss: 0.17400948703289032\n",
      "Epoch: 90/1000 Step: 0 Loss: 0.17365774512290955\n",
      "Epoch: 91/1000 Step: 0 Loss: 0.17344096302986145\n",
      "Epoch: 92/1000 Step: 0 Loss: 0.17302826046943665\n",
      "Epoch: 93/1000 Step: 0 Loss: 0.17291907966136932\n",
      "Epoch: 94/1000 Step: 0 Loss: 0.17265509068965912\n",
      "Epoch: 95/1000 Step: 0 Loss: 0.1724642962217331\n",
      "Epoch: 96/1000 Step: 0 Loss: 0.17228034138679504\n",
      "Epoch: 97/1000 Step: 0 Loss: 0.17209912836551666\n",
      "Epoch: 98/1000 Step: 0 Loss: 0.17180103063583374\n",
      "Epoch: 99/1000 Step: 0 Loss: 0.17161385715007782\n",
      "Epoch: 100/1000 Step: 0 Loss: 0.17126056551933289\n",
      "Epoch: 101/1000 Step: 0 Loss: 0.1709887981414795\n",
      "Epoch: 102/1000 Step: 0 Loss: 0.1708068698644638\n",
      "Epoch: 103/1000 Step: 0 Loss: 0.17065297067165375\n",
      "Epoch: 104/1000 Step: 0 Loss: 0.17053186893463135\n",
      "Epoch: 105/1000 Step: 0 Loss: 0.17033764719963074\n",
      "Epoch: 106/1000 Step: 0 Loss: 0.1701374650001526\n",
      "Epoch: 107/1000 Step: 0 Loss: 0.17000362277030945\n",
      "Epoch: 108/1000 Step: 0 Loss: 0.16983363032341003\n",
      "Epoch: 109/1000 Step: 0 Loss: 0.16974319517612457\n",
      "Epoch: 110/1000 Step: 0 Loss: 0.16957540810108185\n",
      "Epoch: 111/1000 Step: 0 Loss: 0.16943444311618805\n",
      "Epoch: 112/1000 Step: 0 Loss: 0.16932189464569092\n",
      "Epoch: 113/1000 Step: 0 Loss: 0.1691877394914627\n",
      "Epoch: 114/1000 Step: 0 Loss: 0.16905532777309418\n",
      "Epoch: 115/1000 Step: 0 Loss: 0.1689462512731552\n",
      "Epoch: 116/1000 Step: 0 Loss: 0.16885539889335632\n",
      "Epoch: 117/1000 Step: 0 Loss: 0.16876226663589478\n",
      "Epoch: 118/1000 Step: 0 Loss: 0.1686784029006958\n",
      "Epoch: 119/1000 Step: 0 Loss: 0.16861192882061005\n",
      "Epoch: 120/1000 Step: 0 Loss: 0.1685277819633484\n",
      "Epoch: 121/1000 Step: 0 Loss: 0.16842590272426605\n",
      "Epoch: 122/1000 Step: 0 Loss: 0.16836845874786377\n",
      "Epoch: 123/1000 Step: 0 Loss: 0.16828347742557526\n",
      "Epoch: 124/1000 Step: 0 Loss: 0.16821697354316711\n",
      "Epoch: 125/1000 Step: 0 Loss: 0.16815268993377686\n",
      "Epoch: 126/1000 Step: 0 Loss: 0.168106809258461\n",
      "Epoch: 127/1000 Step: 0 Loss: 0.16802451014518738\n",
      "Epoch: 128/1000 Step: 0 Loss: 0.16797295212745667\n",
      "Epoch: 129/1000 Step: 0 Loss: 0.16791577637195587\n",
      "Epoch: 130/1000 Step: 0 Loss: 0.1678747683763504\n",
      "Epoch: 131/1000 Step: 0 Loss: 0.1678309142589569\n",
      "Epoch: 132/1000 Step: 0 Loss: 0.1677859127521515\n",
      "Epoch: 133/1000 Step: 0 Loss: 0.16776123642921448\n",
      "Epoch: 134/1000 Step: 0 Loss: 0.1677296757698059\n",
      "Epoch: 135/1000 Step: 0 Loss: 0.16769744455814362\n",
      "Epoch: 136/1000 Step: 0 Loss: 0.1676698625087738\n",
      "Epoch: 137/1000 Step: 0 Loss: 0.16764500737190247\n",
      "Epoch: 138/1000 Step: 0 Loss: 0.16760945320129395\n",
      "Epoch: 139/1000 Step: 0 Loss: 0.16758932173252106\n",
      "Epoch: 140/1000 Step: 0 Loss: 0.16757886111736298\n",
      "Epoch: 141/1000 Step: 0 Loss: 0.1675492823123932\n",
      "Epoch: 142/1000 Step: 0 Loss: 0.16752594709396362\n",
      "Epoch: 143/1000 Step: 0 Loss: 0.16749513149261475\n",
      "Epoch: 144/1000 Step: 0 Loss: 0.16746020317077637\n",
      "Epoch: 145/1000 Step: 0 Loss: 0.1674155294895172\n",
      "Epoch: 146/1000 Step: 0 Loss: 0.16737671196460724\n",
      "Epoch: 147/1000 Step: 0 Loss: 0.16733068227767944\n",
      "Epoch: 148/1000 Step: 0 Loss: 0.16727295517921448\n",
      "Epoch: 149/1000 Step: 0 Loss: 0.1672261655330658\n",
      "Epoch: 150/1000 Step: 0 Loss: 0.16717538237571716\n",
      "Epoch: 151/1000 Step: 0 Loss: 0.16711530089378357\n",
      "Epoch: 152/1000 Step: 0 Loss: 0.16705726087093353\n",
      "Epoch: 153/1000 Step: 0 Loss: 0.16699764132499695\n",
      "Epoch: 154/1000 Step: 0 Loss: 0.16693808138370514\n",
      "Epoch: 155/1000 Step: 0 Loss: 0.16688504815101624\n",
      "Epoch: 156/1000 Step: 0 Loss: 0.16682037711143494\n",
      "Epoch: 157/1000 Step: 0 Loss: 0.1667667031288147\n",
      "Epoch: 158/1000 Step: 0 Loss: 0.16671472787857056\n",
      "Epoch: 159/1000 Step: 0 Loss: 0.1666724681854248\n",
      "Epoch: 160/1000 Step: 0 Loss: 0.1666334569454193\n",
      "Epoch: 161/1000 Step: 0 Loss: 0.1666036993265152\n",
      "Epoch: 162/1000 Step: 0 Loss: 0.1665753275156021\n",
      "Epoch: 163/1000 Step: 0 Loss: 0.1665496826171875\n",
      "Epoch: 164/1000 Step: 0 Loss: 0.16654355823993683\n",
      "Epoch: 165/1000 Step: 0 Loss: 0.1665378212928772\n",
      "Epoch: 166/1000 Step: 0 Loss: 0.16652806103229523\n",
      "Epoch: 167/1000 Step: 0 Loss: 0.16652368009090424\n",
      "Epoch: 168/1000 Step: 0 Loss: 0.16651175916194916\n",
      "Epoch: 169/1000 Step: 0 Loss: 0.16649621725082397\n",
      "Epoch: 170/1000 Step: 0 Loss: 0.16648247838020325\n",
      "Epoch: 171/1000 Step: 0 Loss: 0.16647355258464813\n",
      "Epoch: 172/1000 Step: 0 Loss: 0.16646601259708405\n",
      "Epoch: 173/1000 Step: 0 Loss: 0.16645662486553192\n",
      "Epoch: 174/1000 Step: 0 Loss: 0.1664489358663559\n",
      "Epoch: 175/1000 Step: 0 Loss: 0.1664380431175232\n",
      "Epoch: 176/1000 Step: 0 Loss: 0.16642795503139496\n",
      "Epoch: 177/1000 Step: 0 Loss: 0.1664166897535324\n",
      "Epoch: 178/1000 Step: 0 Loss: 0.16640453040599823\n",
      "Epoch: 179/1000 Step: 0 Loss: 0.1663929522037506\n",
      "Epoch: 180/1000 Step: 0 Loss: 0.16637712717056274\n",
      "Epoch: 181/1000 Step: 0 Loss: 0.16636072099208832\n",
      "Epoch: 182/1000 Step: 0 Loss: 0.1663452386856079\n",
      "Epoch: 183/1000 Step: 0 Loss: 0.16632820665836334\n",
      "Epoch: 184/1000 Step: 0 Loss: 0.16630494594573975\n",
      "Epoch: 185/1000 Step: 0 Loss: 0.1662885695695877\n",
      "Epoch: 186/1000 Step: 0 Loss: 0.16626614332199097\n",
      "Epoch: 187/1000 Step: 0 Loss: 0.1662500649690628\n",
      "Epoch: 188/1000 Step: 0 Loss: 0.16623519361019135\n",
      "Epoch: 189/1000 Step: 0 Loss: 0.16621512174606323\n",
      "Epoch: 190/1000 Step: 0 Loss: 0.16619625687599182\n",
      "Epoch: 191/1000 Step: 0 Loss: 0.16617311537265778\n",
      "Epoch: 192/1000 Step: 0 Loss: 0.1661522090435028\n",
      "Epoch: 193/1000 Step: 0 Loss: 0.16613157093524933\n",
      "Epoch: 194/1000 Step: 0 Loss: 0.16610656678676605\n",
      "Epoch: 195/1000 Step: 0 Loss: 0.16608797013759613\n",
      "Epoch: 196/1000 Step: 0 Loss: 0.16606728732585907\n",
      "Epoch: 197/1000 Step: 0 Loss: 0.16605249047279358\n",
      "Epoch: 198/1000 Step: 0 Loss: 0.16603821516036987\n",
      "Epoch: 199/1000 Step: 0 Loss: 0.16602657735347748\n",
      "Epoch: 200/1000 Step: 0 Loss: 0.1660178154706955\n",
      "Epoch: 201/1000 Step: 0 Loss: 0.1659989058971405\n",
      "Epoch: 202/1000 Step: 0 Loss: 0.1659855842590332\n",
      "Epoch: 203/1000 Step: 0 Loss: 0.1659751832485199\n",
      "Epoch: 204/1000 Step: 0 Loss: 0.1659584641456604\n",
      "Epoch: 205/1000 Step: 0 Loss: 0.16595087945461273\n",
      "Epoch: 206/1000 Step: 0 Loss: 0.1659460812807083\n",
      "Epoch: 207/1000 Step: 0 Loss: 0.16594143211841583\n",
      "Epoch: 208/1000 Step: 0 Loss: 0.1659376621246338\n",
      "Epoch: 209/1000 Step: 0 Loss: 0.1659330576658249\n",
      "Epoch: 210/1000 Step: 0 Loss: 0.1659252941608429\n",
      "Epoch: 211/1000 Step: 0 Loss: 0.16591694951057434\n",
      "Epoch: 212/1000 Step: 0 Loss: 0.165903702378273\n",
      "Epoch: 213/1000 Step: 0 Loss: 0.16589245200157166\n",
      "Epoch: 214/1000 Step: 0 Loss: 0.1658799648284912\n",
      "Epoch: 215/1000 Step: 0 Loss: 0.16586700081825256\n",
      "Epoch: 216/1000 Step: 0 Loss: 0.1658569574356079\n",
      "Epoch: 217/1000 Step: 0 Loss: 0.16584545373916626\n",
      "Epoch: 218/1000 Step: 0 Loss: 0.16583456099033356\n",
      "Epoch: 219/1000 Step: 0 Loss: 0.16582684218883514\n",
      "Epoch: 220/1000 Step: 0 Loss: 0.16582010686397552\n",
      "Epoch: 221/1000 Step: 0 Loss: 0.16581200063228607\n",
      "Epoch: 222/1000 Step: 0 Loss: 0.1658039540052414\n",
      "Epoch: 223/1000 Step: 0 Loss: 0.16579852998256683\n",
      "Epoch: 224/1000 Step: 0 Loss: 0.16579297184944153\n",
      "Epoch: 225/1000 Step: 0 Loss: 0.165788471698761\n",
      "Epoch: 226/1000 Step: 0 Loss: 0.1657809466123581\n",
      "Epoch: 227/1000 Step: 0 Loss: 0.16577303409576416\n",
      "Epoch: 228/1000 Step: 0 Loss: 0.16576720774173737\n",
      "Epoch: 229/1000 Step: 0 Loss: 0.1657605618238449\n",
      "Epoch: 230/1000 Step: 0 Loss: 0.1657523661851883\n",
      "Epoch: 231/1000 Step: 0 Loss: 0.16574726998806\n",
      "Epoch: 232/1000 Step: 0 Loss: 0.16574357450008392\n",
      "Epoch: 233/1000 Step: 0 Loss: 0.16573822498321533\n",
      "Epoch: 234/1000 Step: 0 Loss: 0.16573333740234375\n",
      "Epoch: 235/1000 Step: 0 Loss: 0.1657295823097229\n",
      "Epoch: 236/1000 Step: 0 Loss: 0.16572582721710205\n",
      "Epoch: 237/1000 Step: 0 Loss: 0.16572460532188416\n",
      "Epoch: 238/1000 Step: 0 Loss: 0.16572383046150208\n",
      "Epoch: 239/1000 Step: 0 Loss: 0.16572371125221252\n",
      "Epoch: 240/1000 Step: 0 Loss: 0.16572371125221252\n",
      "Epoch: 241/1000 Step: 0 Loss: 0.16572371125221252\n",
      "Epoch: 242/1000 Step: 0 Loss: 0.16572369635105133\n",
      "Epoch: 243/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 244/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 245/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 246/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 247/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 248/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 249/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 250/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 251/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 252/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 253/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 254/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 255/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 256/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 257/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 258/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 259/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 260/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 261/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 262/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 263/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 264/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 265/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 266/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 267/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 268/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 269/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 270/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 271/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 272/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 273/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 274/1000 Step: 0 Loss: 0.16572366654872894\n",
      "Epoch: 275/1000 Step: 0 Loss: 0.16572365164756775\n",
      "Epoch: 276/1000 Step: 0 Loss: 0.16572365164756775\n",
      "Epoch: 277/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 278/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 279/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 280/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 281/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 282/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 283/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 284/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 285/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 286/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 287/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 288/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 289/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 290/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 291/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 292/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 293/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 294/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 295/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 296/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 297/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 298/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 299/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 300/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 301/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 302/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 303/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 304/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 305/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 306/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 307/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 308/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 309/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 310/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 311/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 312/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 313/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 314/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 315/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 316/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 317/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 318/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 319/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 320/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 321/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 322/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 323/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 324/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 325/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 326/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 327/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 328/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 329/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 330/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 331/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 332/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 333/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 334/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 335/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 336/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 337/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 338/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 339/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 340/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 341/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 342/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 343/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 344/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 345/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 346/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 347/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 348/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 349/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 350/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 351/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 352/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 353/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 354/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 355/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 356/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 357/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 358/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 359/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 360/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 361/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 362/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 363/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 364/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 365/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 366/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 367/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 368/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 369/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 370/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 371/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 372/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 373/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 374/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 375/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 376/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 377/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 378/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 379/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 380/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 381/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 382/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 383/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 384/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 385/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 386/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 387/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 388/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 389/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 390/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 391/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 392/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 393/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 394/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 395/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 396/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 397/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 398/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 399/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 400/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 401/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 402/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 403/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 404/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 405/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 406/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 407/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 408/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 409/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 410/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 411/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 412/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 413/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 414/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 415/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 416/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 417/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 418/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 419/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 420/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 421/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 422/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 423/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 424/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 425/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 426/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 427/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 428/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 429/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 430/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 431/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 432/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 433/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 434/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 435/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 436/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 437/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 438/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 439/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 440/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 441/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 442/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 443/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 444/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 445/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 446/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 447/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 448/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 449/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 450/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 451/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 452/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 453/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 454/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 455/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 456/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 457/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 458/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 459/1000 Step: 0 Loss: 0.16572363674640656\n",
      "Epoch: 460/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 461/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 462/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 463/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 464/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 465/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 466/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 467/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 468/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 469/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 470/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 471/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 472/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 473/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 474/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 475/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 476/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 477/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 478/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 479/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 480/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 481/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 482/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 483/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 484/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 485/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 486/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 487/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 488/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 489/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 490/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 491/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 492/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 493/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 494/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 495/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 496/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 497/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 498/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 499/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 500/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 501/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 502/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 503/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 504/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 505/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 506/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 507/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 508/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 509/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 510/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 511/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 512/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 513/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 514/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 515/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 516/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 517/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 518/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 519/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 520/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 521/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 522/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 523/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 524/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 525/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 526/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 527/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 528/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 529/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 530/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 531/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 532/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 533/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 534/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 535/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 536/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 537/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 538/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 539/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 540/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 541/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 542/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 543/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 544/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 545/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 546/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 547/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 548/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 549/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 550/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 551/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 552/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 553/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 554/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 555/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 556/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 557/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 558/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 559/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 560/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 561/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 562/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 563/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 564/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 565/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 566/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 567/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 568/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 569/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 570/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 571/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 572/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 573/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 574/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 575/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 576/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 577/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 578/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 579/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 580/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 581/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 582/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 583/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 584/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 585/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 586/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 587/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 588/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 589/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 590/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 591/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 592/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 593/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 594/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 595/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 596/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 597/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 598/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 599/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 600/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 601/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 602/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 603/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 604/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 605/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 606/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 607/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 608/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 609/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 610/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 611/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 612/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 613/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 614/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 615/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 616/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 617/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 618/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 619/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 620/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 621/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 622/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 623/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 624/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 625/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 626/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 627/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 628/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 629/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 630/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 631/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 632/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 633/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 634/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 635/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 636/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 637/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 638/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 639/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 640/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 641/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 642/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 643/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 644/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 645/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 646/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 647/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 648/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 649/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 650/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 651/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 652/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 653/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 654/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 655/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 656/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 657/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 658/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 659/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 660/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 661/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 662/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 663/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 664/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 665/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 666/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 667/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 668/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 669/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 670/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 671/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 672/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 673/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 674/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 675/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 676/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 677/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 678/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 679/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 680/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 681/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 682/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 683/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 684/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 685/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 686/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 687/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 688/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 689/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 690/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 691/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 692/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 693/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 694/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 695/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 696/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 697/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 698/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 699/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 700/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 701/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 702/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 703/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 704/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 705/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 706/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 707/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 708/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 709/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 710/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 711/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 712/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 713/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 714/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 715/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 716/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 717/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 718/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 719/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 720/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 721/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 722/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 723/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 724/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 725/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 726/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 727/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 728/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 729/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 730/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 731/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 732/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 733/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 734/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 735/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 736/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 737/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 738/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 739/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 740/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 741/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 742/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 743/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 744/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 745/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 746/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 747/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 748/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 749/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 750/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 751/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 752/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 753/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 754/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 755/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 756/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 757/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 758/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 759/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 760/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 761/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 762/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 763/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 764/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 765/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 766/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 767/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 768/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 769/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 770/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 771/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 772/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 773/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 774/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 775/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 776/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 777/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 778/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 779/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 780/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 781/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 782/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 783/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 784/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 785/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 786/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 787/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 788/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 789/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 790/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 791/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 792/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 793/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 794/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 795/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 796/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 797/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 798/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 799/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 800/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 801/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 802/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 803/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 804/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 805/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 806/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 807/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 808/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 809/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 810/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 811/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 812/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 813/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 814/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 815/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 816/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 817/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 818/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 819/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 820/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 821/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 822/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 823/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 824/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 825/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 826/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 827/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 828/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 829/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 830/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 831/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 832/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 833/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 834/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 835/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 836/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 837/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 838/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 839/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 840/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 841/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 842/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 843/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 844/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 845/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 846/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 847/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 848/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 849/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 850/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 851/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 852/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 853/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 854/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 855/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 856/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 857/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 858/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 859/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 860/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 861/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 862/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 863/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 864/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 865/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 866/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 867/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 868/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 869/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 870/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 871/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 872/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 873/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 874/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 875/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 876/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 877/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 878/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 879/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 880/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 881/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 882/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 883/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 884/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 885/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 886/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 887/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 888/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 889/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 890/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 891/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 892/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 893/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 894/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 895/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 896/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 897/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 898/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 899/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 900/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 901/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 902/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 903/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 904/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 905/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 906/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 907/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 908/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 909/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 910/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 911/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 912/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 913/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 914/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 915/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 916/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 917/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 918/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 919/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 920/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 921/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 922/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 923/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 924/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 925/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 926/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 927/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 928/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 929/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 930/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 931/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 932/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 933/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 934/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 935/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 936/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 937/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 938/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 939/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 940/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 941/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 942/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 943/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 944/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 945/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 946/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 947/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 948/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 949/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 950/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 951/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 952/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 953/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 954/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 955/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 956/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 957/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 958/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 959/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 960/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 961/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 962/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 963/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 964/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 965/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 966/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 967/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 968/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 969/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 970/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 971/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 972/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 973/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 974/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 975/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 976/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 977/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 978/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 979/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 980/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 981/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 982/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 983/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 984/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 985/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 986/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 987/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 988/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 989/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 990/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 991/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 992/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 993/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 994/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 995/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 996/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 997/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 998/1000 Step: 0 Loss: 0.16572362184524536\n",
      "Epoch: 999/1000 Step: 0 Loss: 0.16572362184524536\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "optim = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "model = model.to(torch.float32)\n",
    "\n",
    "weights = []\n",
    "params = list(model.parameters()).copy()\n",
    "for i in range(len(params)):\n",
    "    weights += params[i].flatten().detach().tolist()\n",
    "weights = np.array(weights)\n",
    "\n",
    "sample = iter(train_loader)\n",
    "x,y = next(sample)\n",
    "x = x.to(torch.float32)\n",
    "y = y.to(torch.float32)\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "# model.l3(model.l2(model.l1(x)))\n",
    "\n",
    "c1,c2 = 0.3,0.3 #cog,soc\n",
    "l = [particle(0.8,c1,c2,weights.shape[0],model,loss,x,y) for i in range(500)]\n",
    "gbest = np.ones(weights.shape[0]+1)*10000\n",
    "calc_gbest(l)\n",
    "\n",
    "def iterate():\n",
    "    calc_gbest(l)\n",
    "    \n",
    "    for i in l:\n",
    "        i.iterate(gbest)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "        j.backward()\n",
    "        # optim.step()\n",
    "\n",
    "        iterate()\n",
    "\n",
    "        losses += [gbest[0].item()]\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs} Step: {i} Loss: {gbest[0].item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.7\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "\n",
    "    for x,y in test_loader:\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        # d = c + model.l3.out_features\n",
    "\n",
    "        position = gbest[1:]\n",
    "\n",
    "        weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "        bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "        weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "        bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "        model.l1.weight = nn.Parameter(weights1)\n",
    "        model.l1.bias = nn.Parameter(bias1)\n",
    "        model.l3.weight = nn.Parameter(weights2)\n",
    "        model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        pred = torch.round(output).reshape(1,-1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (pred==y).sum().item()\n",
    "        # print(n_correct)\n",
    "    acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
