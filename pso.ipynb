{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data and Loading into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "# df1 = df[df[\"Personal Loan\"]==1]\n",
    "# df0 = df[df[\"Personal Loan\"]==0][:480]\n",
    "# df = pd.concat((df0,df1))\n",
    "# df = df.sample(frac=1)\n",
    "df.drop(\"ID\",axis=1,inplace=True)\n",
    "\n",
    "features = df.drop(\"Personal Loan\",axis=1).columns\n",
    "target = \"Personal Loan\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(df[features],df[target],test_size=0.2)\n",
    "\n",
    "traindb = pd.concat((trainx,trainy),axis=1)\n",
    "testdb = pd.concat((testx,testy),axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "class db(Dataset):\n",
    "    def __init__(self,target,features,df) -> None:\n",
    "        self.y  = torch.tensor(df[target].values)\n",
    "        self.x = torch.tensor(df[features].values)\n",
    "        self.x = scaler.fit_transform(self.x)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "\n",
    "traindbt = db(target,features,traindb)\n",
    "testdbt = db(target,features,testdb)\n",
    "\n",
    "train_loader = DataLoader(traindbt,batch_size=len(traindb),shuffle=True)\n",
    "test_loader = DataLoader(testdbt,batch_size=len(testdb),shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Particle Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = 10\n",
    "\n",
    "def fitness(position,model,loss,x,y):\n",
    "    a = model.l1.in_features * model.l1.out_features\n",
    "    b = a + model.l1.out_features\n",
    "    c = b + (model.l3.in_features * model.l3.out_features)\n",
    "    # d = c + model.l3.out_features\n",
    "\n",
    "    weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "    bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "    weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "    bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "    model.l1.weight = nn.Parameter(weights1)\n",
    "    model.l1.bias = nn.Parameter(bias1)\n",
    "    model.l3.weight = nn.Parameter(weights2)\n",
    "    model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "    y_ = model(x)\n",
    "    j = loss(y_,y.unsqueeze(-1))\n",
    "    \n",
    "    return j\n",
    "\n",
    "class particle:\n",
    "    def __init__(self,w,c1,c2,l,model,loss,x,y):\n",
    "        self.l = l\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.x, self.y = x,y\n",
    "        self.position = np.random.random(l)*20-xlim\n",
    "        self.velocity = np.random.random(l)\n",
    "        self.fitness = fitness(self.position,self.model,self.loss,self.x,self.y)\n",
    "        self.pbest = [self.fitness] + [i for i in self.position]\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.r1, self.r2 = np.random.random(2)\n",
    "\n",
    "    def calc_velocity(self,gbest):\n",
    "        soc = self.calc_soc(gbest)\n",
    "        cog = self.calc_cog()\n",
    "\n",
    "        self.velocity = self.w * self.velocity + soc + cog\n",
    "    \n",
    "    def calc_soc(self,gbest):\n",
    "        return self.c1 * self.r1 * (np.array([i for i in gbest][1:]) - self.position)\n",
    "    \n",
    "    def calc_cog(self):\n",
    "        return self.c2 * self.r2 * (np.array([i for i in self.pbest][1:]) - self.position)\n",
    "    \n",
    "    def iterate(self,gbest):\n",
    "\n",
    "        self.calc_velocity(gbest)\n",
    "\n",
    "        self.position += self.velocity\n",
    "\n",
    "        self.position = np.clip(self.position,-xlim,xlim)\n",
    "\n",
    "        self.fitness = fitness(self.position,self.model,self.loss,self.x,self.y)\n",
    "\n",
    "        if self.fitness<self.pbest[0]:\n",
    "            self.pbest[0] = self.fitness\n",
    "            for i in range(len(self.position)):\n",
    "                self.pbest[i+1] = self.position[i]\n",
    "        \n",
    "        self.r1, self.r2 = np.random.rand(2)*2\n",
    "\n",
    "def calc_gbest(l):\n",
    "    global gbest\n",
    "    for i in l:\n",
    "        if i.pbest[0]<gbest[0]:\n",
    "            gbest[0] = i.pbest[0]\n",
    "            for j in range(len(i.position)):\n",
    "                gbest[j+1] = i.position[j]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network (PyTorch)\n",
    "### Neural Network Architecture:\n",
    "- Input Layer: 12\n",
    "- Hidden Layer: 16\n",
    "- Output Layer: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(net,self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(12,16)\n",
    "        self.l2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(16,1)\n",
    "        self.l4 = nn.Sigmoid()\n",
    "        # self.l5 = nn.Linear(20,1)\n",
    "        # self.l6 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        x = X\n",
    "\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        # x = self.l5(x)\n",
    "        # x = self.l6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Step: 0 Loss: 9.048264503479004\n",
      "Epoch: 1/1000 Step: 0 Loss: 8.628633499145508\n",
      "Epoch: 2/1000 Step: 0 Loss: 5.549644947052002\n",
      "Epoch: 3/1000 Step: 0 Loss: 4.846626281738281\n",
      "Epoch: 4/1000 Step: 0 Loss: 4.291600227355957\n",
      "Epoch: 5/1000 Step: 0 Loss: 3.3968591690063477\n",
      "Epoch: 6/1000 Step: 0 Loss: 2.1155102252960205\n",
      "Epoch: 7/1000 Step: 0 Loss: 1.4341439008712769\n",
      "Epoch: 8/1000 Step: 0 Loss: 1.1975922584533691\n",
      "Epoch: 9/1000 Step: 0 Loss: 0.8849673867225647\n",
      "Epoch: 10/1000 Step: 0 Loss: 0.7039817571640015\n",
      "Epoch: 11/1000 Step: 0 Loss: 0.6727827191352844\n",
      "Epoch: 12/1000 Step: 0 Loss: 0.5480109453201294\n",
      "Epoch: 13/1000 Step: 0 Loss: 0.4206254780292511\n",
      "Epoch: 14/1000 Step: 0 Loss: 0.4206254780292511\n",
      "Epoch: 15/1000 Step: 0 Loss: 0.4023066759109497\n",
      "Epoch: 16/1000 Step: 0 Loss: 0.3771248459815979\n",
      "Epoch: 17/1000 Step: 0 Loss: 0.37130993604660034\n",
      "Epoch: 18/1000 Step: 0 Loss: 0.3433789014816284\n",
      "Epoch: 19/1000 Step: 0 Loss: 0.34271806478500366\n",
      "Epoch: 20/1000 Step: 0 Loss: 0.3333728611469269\n",
      "Epoch: 21/1000 Step: 0 Loss: 0.32457005977630615\n",
      "Epoch: 22/1000 Step: 0 Loss: 0.3073984980583191\n",
      "Epoch: 23/1000 Step: 0 Loss: 0.3031212389469147\n",
      "Epoch: 24/1000 Step: 0 Loss: 0.29903918504714966\n",
      "Epoch: 25/1000 Step: 0 Loss: 0.29086732864379883\n",
      "Epoch: 26/1000 Step: 0 Loss: 0.2840842306613922\n",
      "Epoch: 27/1000 Step: 0 Loss: 0.2789436876773834\n",
      "Epoch: 28/1000 Step: 0 Loss: 0.27625665068626404\n",
      "Epoch: 29/1000 Step: 0 Loss: 0.2644197642803192\n",
      "Epoch: 30/1000 Step: 0 Loss: 0.2489510327577591\n",
      "Epoch: 31/1000 Step: 0 Loss: 0.24307376146316528\n",
      "Epoch: 32/1000 Step: 0 Loss: 0.23745594918727875\n",
      "Epoch: 33/1000 Step: 0 Loss: 0.21964049339294434\n",
      "Epoch: 34/1000 Step: 0 Loss: 0.21713659167289734\n",
      "Epoch: 35/1000 Step: 0 Loss: 0.20773401856422424\n",
      "Epoch: 36/1000 Step: 0 Loss: 0.199983611702919\n",
      "Epoch: 37/1000 Step: 0 Loss: 0.1976846307516098\n",
      "Epoch: 38/1000 Step: 0 Loss: 0.19349023699760437\n",
      "Epoch: 39/1000 Step: 0 Loss: 0.19257083535194397\n",
      "Epoch: 40/1000 Step: 0 Loss: 0.19012920558452606\n",
      "Epoch: 41/1000 Step: 0 Loss: 0.18765293061733246\n",
      "Epoch: 42/1000 Step: 0 Loss: 0.18566542863845825\n",
      "Epoch: 43/1000 Step: 0 Loss: 0.18238429725170135\n",
      "Epoch: 44/1000 Step: 0 Loss: 0.17933692038059235\n",
      "Epoch: 45/1000 Step: 0 Loss: 0.17675042152404785\n",
      "Epoch: 46/1000 Step: 0 Loss: 0.1747070699930191\n",
      "Epoch: 47/1000 Step: 0 Loss: 0.1717652827501297\n",
      "Epoch: 48/1000 Step: 0 Loss: 0.17143969237804413\n",
      "Epoch: 49/1000 Step: 0 Loss: 0.17059819400310516\n",
      "Epoch: 50/1000 Step: 0 Loss: 0.16916720569133759\n",
      "Epoch: 51/1000 Step: 0 Loss: 0.168415367603302\n",
      "Epoch: 52/1000 Step: 0 Loss: 0.16573436558246613\n",
      "Epoch: 53/1000 Step: 0 Loss: 0.16497500240802765\n",
      "Epoch: 54/1000 Step: 0 Loss: 0.16378755867481232\n",
      "Epoch: 55/1000 Step: 0 Loss: 0.16308926045894623\n",
      "Epoch: 56/1000 Step: 0 Loss: 0.16177570819854736\n",
      "Epoch: 57/1000 Step: 0 Loss: 0.160050168633461\n",
      "Epoch: 58/1000 Step: 0 Loss: 0.15882237255573273\n",
      "Epoch: 59/1000 Step: 0 Loss: 0.1568358987569809\n",
      "Epoch: 60/1000 Step: 0 Loss: 0.15581519901752472\n",
      "Epoch: 61/1000 Step: 0 Loss: 0.15459145605564117\n",
      "Epoch: 62/1000 Step: 0 Loss: 0.1533152163028717\n",
      "Epoch: 63/1000 Step: 0 Loss: 0.15249541401863098\n",
      "Epoch: 64/1000 Step: 0 Loss: 0.15167388319969177\n",
      "Epoch: 65/1000 Step: 0 Loss: 0.1505599319934845\n",
      "Epoch: 66/1000 Step: 0 Loss: 0.14951777458190918\n",
      "Epoch: 67/1000 Step: 0 Loss: 0.14915233850479126\n",
      "Epoch: 68/1000 Step: 0 Loss: 0.14860966801643372\n",
      "Epoch: 69/1000 Step: 0 Loss: 0.14811444282531738\n",
      "Epoch: 70/1000 Step: 0 Loss: 0.14771826565265656\n",
      "Epoch: 71/1000 Step: 0 Loss: 0.14756722748279572\n",
      "Epoch: 72/1000 Step: 0 Loss: 0.14705535769462585\n",
      "Epoch: 73/1000 Step: 0 Loss: 0.14657534658908844\n",
      "Epoch: 74/1000 Step: 0 Loss: 0.14631754159927368\n",
      "Epoch: 75/1000 Step: 0 Loss: 0.14606350660324097\n",
      "Epoch: 76/1000 Step: 0 Loss: 0.14599744975566864\n",
      "Epoch: 77/1000 Step: 0 Loss: 0.14575771987438202\n",
      "Epoch: 78/1000 Step: 0 Loss: 0.14550407230854034\n",
      "Epoch: 79/1000 Step: 0 Loss: 0.14527815580368042\n",
      "Epoch: 80/1000 Step: 0 Loss: 0.1448899507522583\n",
      "Epoch: 81/1000 Step: 0 Loss: 0.14455054700374603\n",
      "Epoch: 82/1000 Step: 0 Loss: 0.14423765242099762\n",
      "Epoch: 83/1000 Step: 0 Loss: 0.1440308839082718\n",
      "Epoch: 84/1000 Step: 0 Loss: 0.14380651712417603\n",
      "Epoch: 85/1000 Step: 0 Loss: 0.14357301592826843\n",
      "Epoch: 86/1000 Step: 0 Loss: 0.14325478672981262\n",
      "Epoch: 87/1000 Step: 0 Loss: 0.14305570721626282\n",
      "Epoch: 88/1000 Step: 0 Loss: 0.1427219957113266\n",
      "Epoch: 89/1000 Step: 0 Loss: 0.1424233466386795\n",
      "Epoch: 90/1000 Step: 0 Loss: 0.1421373039484024\n",
      "Epoch: 91/1000 Step: 0 Loss: 0.14191672205924988\n",
      "Epoch: 92/1000 Step: 0 Loss: 0.14175115525722504\n",
      "Epoch: 93/1000 Step: 0 Loss: 0.1414908766746521\n",
      "Epoch: 94/1000 Step: 0 Loss: 0.141264408826828\n",
      "Epoch: 95/1000 Step: 0 Loss: 0.14102378487586975\n",
      "Epoch: 96/1000 Step: 0 Loss: 0.1408083438873291\n",
      "Epoch: 97/1000 Step: 0 Loss: 0.14045041799545288\n",
      "Epoch: 98/1000 Step: 0 Loss: 0.14023248851299286\n",
      "Epoch: 99/1000 Step: 0 Loss: 0.13999813795089722\n",
      "Epoch: 100/1000 Step: 0 Loss: 0.1398332118988037\n",
      "Epoch: 101/1000 Step: 0 Loss: 0.13970953226089478\n",
      "Epoch: 102/1000 Step: 0 Loss: 0.13966673612594604\n",
      "Epoch: 103/1000 Step: 0 Loss: 0.13958539068698883\n",
      "Epoch: 104/1000 Step: 0 Loss: 0.13948369026184082\n",
      "Epoch: 105/1000 Step: 0 Loss: 0.13937003910541534\n",
      "Epoch: 106/1000 Step: 0 Loss: 0.13929396867752075\n",
      "Epoch: 107/1000 Step: 0 Loss: 0.1391773372888565\n",
      "Epoch: 108/1000 Step: 0 Loss: 0.13906097412109375\n",
      "Epoch: 109/1000 Step: 0 Loss: 0.13893648982048035\n",
      "Epoch: 110/1000 Step: 0 Loss: 0.13880997896194458\n",
      "Epoch: 111/1000 Step: 0 Loss: 0.13866554200649261\n",
      "Epoch: 112/1000 Step: 0 Loss: 0.13854649662971497\n",
      "Epoch: 113/1000 Step: 0 Loss: 0.1383972018957138\n",
      "Epoch: 114/1000 Step: 0 Loss: 0.13826116919517517\n",
      "Epoch: 115/1000 Step: 0 Loss: 0.13811925053596497\n",
      "Epoch: 116/1000 Step: 0 Loss: 0.13795273005962372\n",
      "Epoch: 117/1000 Step: 0 Loss: 0.13781782984733582\n",
      "Epoch: 118/1000 Step: 0 Loss: 0.1376664638519287\n",
      "Epoch: 119/1000 Step: 0 Loss: 0.13753819465637207\n",
      "Epoch: 120/1000 Step: 0 Loss: 0.1374286711215973\n",
      "Epoch: 121/1000 Step: 0 Loss: 0.13734453916549683\n",
      "Epoch: 122/1000 Step: 0 Loss: 0.13729232549667358\n",
      "Epoch: 123/1000 Step: 0 Loss: 0.13723121583461761\n",
      "Epoch: 124/1000 Step: 0 Loss: 0.13716959953308105\n",
      "Epoch: 125/1000 Step: 0 Loss: 0.13713012635707855\n",
      "Epoch: 126/1000 Step: 0 Loss: 0.13708189129829407\n",
      "Epoch: 127/1000 Step: 0 Loss: 0.13704492151737213\n",
      "Epoch: 128/1000 Step: 0 Loss: 0.13702870905399323\n",
      "Epoch: 129/1000 Step: 0 Loss: 0.13700026273727417\n",
      "Epoch: 130/1000 Step: 0 Loss: 0.13696689903736115\n",
      "Epoch: 131/1000 Step: 0 Loss: 0.13693515956401825\n",
      "Epoch: 132/1000 Step: 0 Loss: 0.13690373301506042\n",
      "Epoch: 133/1000 Step: 0 Loss: 0.1368705779314041\n",
      "Epoch: 134/1000 Step: 0 Loss: 0.13684780895709991\n",
      "Epoch: 135/1000 Step: 0 Loss: 0.13683028519153595\n",
      "Epoch: 136/1000 Step: 0 Loss: 0.13681350648403168\n",
      "Epoch: 137/1000 Step: 0 Loss: 0.13679490983486176\n",
      "Epoch: 138/1000 Step: 0 Loss: 0.13678188621997833\n",
      "Epoch: 139/1000 Step: 0 Loss: 0.13674600422382355\n",
      "Epoch: 140/1000 Step: 0 Loss: 0.13671672344207764\n",
      "Epoch: 141/1000 Step: 0 Loss: 0.13668741285800934\n",
      "Epoch: 142/1000 Step: 0 Loss: 0.1366594433784485\n",
      "Epoch: 143/1000 Step: 0 Loss: 0.13663579523563385\n",
      "Epoch: 144/1000 Step: 0 Loss: 0.13661514222621918\n",
      "Epoch: 145/1000 Step: 0 Loss: 0.13659031689167023\n",
      "Epoch: 146/1000 Step: 0 Loss: 0.13656114041805267\n",
      "Epoch: 147/1000 Step: 0 Loss: 0.1365213841199875\n",
      "Epoch: 148/1000 Step: 0 Loss: 0.1364896297454834\n",
      "Epoch: 149/1000 Step: 0 Loss: 0.13646182417869568\n",
      "Epoch: 150/1000 Step: 0 Loss: 0.13642694056034088\n",
      "Epoch: 151/1000 Step: 0 Loss: 0.1363910585641861\n",
      "Epoch: 152/1000 Step: 0 Loss: 0.13636840879917145\n",
      "Epoch: 153/1000 Step: 0 Loss: 0.13633884489536285\n",
      "Epoch: 154/1000 Step: 0 Loss: 0.1363069713115692\n",
      "Epoch: 155/1000 Step: 0 Loss: 0.13627327978610992\n",
      "Epoch: 156/1000 Step: 0 Loss: 0.13623468577861786\n",
      "Epoch: 157/1000 Step: 0 Loss: 0.13619756698608398\n",
      "Epoch: 158/1000 Step: 0 Loss: 0.13615886867046356\n",
      "Epoch: 159/1000 Step: 0 Loss: 0.1361110806465149\n",
      "Epoch: 160/1000 Step: 0 Loss: 0.1360691487789154\n",
      "Epoch: 161/1000 Step: 0 Loss: 0.13602732121944427\n",
      "Epoch: 162/1000 Step: 0 Loss: 0.1359926164150238\n",
      "Epoch: 163/1000 Step: 0 Loss: 0.1359403282403946\n",
      "Epoch: 164/1000 Step: 0 Loss: 0.1358967423439026\n",
      "Epoch: 165/1000 Step: 0 Loss: 0.13586339354515076\n",
      "Epoch: 166/1000 Step: 0 Loss: 0.13582853972911835\n",
      "Epoch: 167/1000 Step: 0 Loss: 0.13580985367298126\n",
      "Epoch: 168/1000 Step: 0 Loss: 0.13579265773296356\n",
      "Epoch: 169/1000 Step: 0 Loss: 0.1357754021883011\n",
      "Epoch: 170/1000 Step: 0 Loss: 0.13575726747512817\n",
      "Epoch: 171/1000 Step: 0 Loss: 0.1357457935810089\n",
      "Epoch: 172/1000 Step: 0 Loss: 0.13574405014514923\n",
      "Epoch: 173/1000 Step: 0 Loss: 0.13572430610656738\n",
      "Epoch: 174/1000 Step: 0 Loss: 0.13571079075336456\n",
      "Epoch: 175/1000 Step: 0 Loss: 0.13569703698158264\n",
      "Epoch: 176/1000 Step: 0 Loss: 0.13567550480365753\n",
      "Epoch: 177/1000 Step: 0 Loss: 0.13565465807914734\n",
      "Epoch: 178/1000 Step: 0 Loss: 0.13563324511051178\n",
      "Epoch: 179/1000 Step: 0 Loss: 0.1356106847524643\n",
      "Epoch: 180/1000 Step: 0 Loss: 0.13559255003929138\n",
      "Epoch: 181/1000 Step: 0 Loss: 0.1355782002210617\n",
      "Epoch: 182/1000 Step: 0 Loss: 0.1355711817741394\n",
      "Epoch: 183/1000 Step: 0 Loss: 0.13556304574012756\n",
      "Epoch: 184/1000 Step: 0 Loss: 0.1355588138103485\n",
      "Epoch: 185/1000 Step: 0 Loss: 0.1355559378862381\n",
      "Epoch: 186/1000 Step: 0 Loss: 0.13554830849170685\n",
      "Epoch: 187/1000 Step: 0 Loss: 0.13554036617279053\n",
      "Epoch: 188/1000 Step: 0 Loss: 0.1355310082435608\n",
      "Epoch: 189/1000 Step: 0 Loss: 0.1355217546224594\n",
      "Epoch: 190/1000 Step: 0 Loss: 0.13551251590251923\n",
      "Epoch: 191/1000 Step: 0 Loss: 0.1355062872171402\n",
      "Epoch: 192/1000 Step: 0 Loss: 0.13549984991550446\n",
      "Epoch: 193/1000 Step: 0 Loss: 0.13549388945102692\n",
      "Epoch: 194/1000 Step: 0 Loss: 0.13548745214939117\n",
      "Epoch: 195/1000 Step: 0 Loss: 0.13548260927200317\n",
      "Epoch: 196/1000 Step: 0 Loss: 0.13547681272029877\n",
      "Epoch: 197/1000 Step: 0 Loss: 0.13547033071517944\n",
      "Epoch: 198/1000 Step: 0 Loss: 0.13546407222747803\n",
      "Epoch: 199/1000 Step: 0 Loss: 0.13545885682106018\n",
      "Epoch: 200/1000 Step: 0 Loss: 0.13545426726341248\n",
      "Epoch: 201/1000 Step: 0 Loss: 0.13544867932796478\n",
      "Epoch: 202/1000 Step: 0 Loss: 0.13544362783432007\n",
      "Epoch: 203/1000 Step: 0 Loss: 0.13543842732906342\n",
      "Epoch: 204/1000 Step: 0 Loss: 0.13543210923671722\n",
      "Epoch: 205/1000 Step: 0 Loss: 0.1354278177022934\n",
      "Epoch: 206/1000 Step: 0 Loss: 0.1354215294122696\n",
      "Epoch: 207/1000 Step: 0 Loss: 0.1354154497385025\n",
      "Epoch: 208/1000 Step: 0 Loss: 0.13540923595428467\n",
      "Epoch: 209/1000 Step: 0 Loss: 0.13540372252464294\n",
      "Epoch: 210/1000 Step: 0 Loss: 0.13539694249629974\n",
      "Epoch: 211/1000 Step: 0 Loss: 0.1353900283575058\n",
      "Epoch: 212/1000 Step: 0 Loss: 0.13538390398025513\n",
      "Epoch: 213/1000 Step: 0 Loss: 0.13537482917308807\n",
      "Epoch: 214/1000 Step: 0 Loss: 0.13536718487739563\n",
      "Epoch: 215/1000 Step: 0 Loss: 0.13536010682582855\n",
      "Epoch: 216/1000 Step: 0 Loss: 0.13535262644290924\n",
      "Epoch: 217/1000 Step: 0 Loss: 0.13534726202487946\n",
      "Epoch: 218/1000 Step: 0 Loss: 0.13534164428710938\n",
      "Epoch: 219/1000 Step: 0 Loss: 0.13533852994441986\n",
      "Epoch: 220/1000 Step: 0 Loss: 0.13533616065979004\n",
      "Epoch: 221/1000 Step: 0 Loss: 0.13533563911914825\n",
      "Epoch: 222/1000 Step: 0 Loss: 0.13533556461334229\n",
      "Epoch: 223/1000 Step: 0 Loss: 0.13533556461334229\n",
      "Epoch: 224/1000 Step: 0 Loss: 0.1353355199098587\n",
      "Epoch: 225/1000 Step: 0 Loss: 0.13533520698547363\n",
      "Epoch: 226/1000 Step: 0 Loss: 0.13533495366573334\n",
      "Epoch: 227/1000 Step: 0 Loss: 0.13533484935760498\n",
      "Epoch: 228/1000 Step: 0 Loss: 0.135334774851799\n",
      "Epoch: 229/1000 Step: 0 Loss: 0.13533470034599304\n",
      "Epoch: 230/1000 Step: 0 Loss: 0.1353345811367035\n",
      "Epoch: 231/1000 Step: 0 Loss: 0.13533450663089752\n",
      "Epoch: 232/1000 Step: 0 Loss: 0.13533441722393036\n",
      "Epoch: 233/1000 Step: 0 Loss: 0.13533438742160797\n",
      "Epoch: 234/1000 Step: 0 Loss: 0.13533428311347961\n",
      "Epoch: 235/1000 Step: 0 Loss: 0.13533413410186768\n",
      "Epoch: 236/1000 Step: 0 Loss: 0.13533401489257812\n",
      "Epoch: 237/1000 Step: 0 Loss: 0.13533392548561096\n",
      "Epoch: 238/1000 Step: 0 Loss: 0.13533371686935425\n",
      "Epoch: 239/1000 Step: 0 Loss: 0.1353335827589035\n",
      "Epoch: 240/1000 Step: 0 Loss: 0.13533346354961395\n",
      "Epoch: 241/1000 Step: 0 Loss: 0.1353333592414856\n",
      "Epoch: 242/1000 Step: 0 Loss: 0.13533319532871246\n",
      "Epoch: 243/1000 Step: 0 Loss: 0.13533303141593933\n",
      "Epoch: 244/1000 Step: 0 Loss: 0.135332852602005\n",
      "Epoch: 245/1000 Step: 0 Loss: 0.13533271849155426\n",
      "Epoch: 246/1000 Step: 0 Loss: 0.13533252477645874\n",
      "Epoch: 247/1000 Step: 0 Loss: 0.135332390666008\n",
      "Epoch: 248/1000 Step: 0 Loss: 0.13533222675323486\n",
      "Epoch: 249/1000 Step: 0 Loss: 0.13533204793930054\n",
      "Epoch: 250/1000 Step: 0 Loss: 0.1353318840265274\n",
      "Epoch: 251/1000 Step: 0 Loss: 0.13533173501491547\n",
      "Epoch: 252/1000 Step: 0 Loss: 0.13533157110214233\n",
      "Epoch: 253/1000 Step: 0 Loss: 0.13533137738704681\n",
      "Epoch: 254/1000 Step: 0 Loss: 0.13533121347427368\n",
      "Epoch: 255/1000 Step: 0 Loss: 0.13533098995685577\n",
      "Epoch: 256/1000 Step: 0 Loss: 0.13533076643943787\n",
      "Epoch: 257/1000 Step: 0 Loss: 0.13533052802085876\n",
      "Epoch: 258/1000 Step: 0 Loss: 0.13533031940460205\n",
      "Epoch: 259/1000 Step: 0 Loss: 0.13533006608486176\n",
      "Epoch: 260/1000 Step: 0 Loss: 0.13532967865467072\n",
      "Epoch: 261/1000 Step: 0 Loss: 0.1353292018175125\n",
      "Epoch: 262/1000 Step: 0 Loss: 0.13532885909080505\n",
      "Epoch: 263/1000 Step: 0 Loss: 0.13532845675945282\n",
      "Epoch: 264/1000 Step: 0 Loss: 0.1353280246257782\n",
      "Epoch: 265/1000 Step: 0 Loss: 0.1353275626897812\n",
      "Epoch: 266/1000 Step: 0 Loss: 0.13532723486423492\n",
      "Epoch: 267/1000 Step: 0 Loss: 0.13532687723636627\n",
      "Epoch: 268/1000 Step: 0 Loss: 0.13532660901546478\n",
      "Epoch: 269/1000 Step: 0 Loss: 0.13532638549804688\n",
      "Epoch: 270/1000 Step: 0 Loss: 0.13532638549804688\n",
      "Epoch: 271/1000 Step: 0 Loss: 0.13532638549804688\n",
      "Epoch: 272/1000 Step: 0 Loss: 0.13532625138759613\n",
      "Epoch: 273/1000 Step: 0 Loss: 0.13532613217830658\n",
      "Epoch: 274/1000 Step: 0 Loss: 0.13532598316669464\n",
      "Epoch: 275/1000 Step: 0 Loss: 0.13532589375972748\n",
      "Epoch: 276/1000 Step: 0 Loss: 0.1353258192539215\n",
      "Epoch: 277/1000 Step: 0 Loss: 0.13532570004463196\n",
      "Epoch: 278/1000 Step: 0 Loss: 0.135325625538826\n",
      "Epoch: 279/1000 Step: 0 Loss: 0.1353256106376648\n",
      "Epoch: 280/1000 Step: 0 Loss: 0.1353255659341812\n",
      "Epoch: 281/1000 Step: 0 Loss: 0.13532555103302002\n",
      "Epoch: 282/1000 Step: 0 Loss: 0.13532546162605286\n",
      "Epoch: 283/1000 Step: 0 Loss: 0.1353253573179245\n",
      "Epoch: 284/1000 Step: 0 Loss: 0.13532529771327972\n",
      "Epoch: 285/1000 Step: 0 Loss: 0.13532517850399017\n",
      "Epoch: 286/1000 Step: 0 Loss: 0.135325089097023\n",
      "Epoch: 287/1000 Step: 0 Loss: 0.13532495498657227\n",
      "Epoch: 288/1000 Step: 0 Loss: 0.13532483577728271\n",
      "Epoch: 289/1000 Step: 0 Loss: 0.13532467186450958\n",
      "Epoch: 290/1000 Step: 0 Loss: 0.13532452285289764\n",
      "Epoch: 291/1000 Step: 0 Loss: 0.1353243738412857\n",
      "Epoch: 292/1000 Step: 0 Loss: 0.13532420992851257\n",
      "Epoch: 293/1000 Step: 0 Loss: 0.13532401621341705\n",
      "Epoch: 294/1000 Step: 0 Loss: 0.13532382249832153\n",
      "Epoch: 295/1000 Step: 0 Loss: 0.135323628783226\n",
      "Epoch: 296/1000 Step: 0 Loss: 0.1353234201669693\n",
      "Epoch: 297/1000 Step: 0 Loss: 0.135323166847229\n",
      "Epoch: 298/1000 Step: 0 Loss: 0.13532297313213348\n",
      "Epoch: 299/1000 Step: 0 Loss: 0.13532280921936035\n",
      "Epoch: 300/1000 Step: 0 Loss: 0.13532263040542603\n",
      "Epoch: 301/1000 Step: 0 Loss: 0.13532251119613647\n",
      "Epoch: 302/1000 Step: 0 Loss: 0.13532240688800812\n",
      "Epoch: 303/1000 Step: 0 Loss: 0.13532233238220215\n",
      "Epoch: 304/1000 Step: 0 Loss: 0.13532230257987976\n",
      "Epoch: 305/1000 Step: 0 Loss: 0.13532228767871857\n",
      "Epoch: 306/1000 Step: 0 Loss: 0.13532228767871857\n",
      "Epoch: 307/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 308/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 309/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 310/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 311/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 312/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 313/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 314/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 315/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 316/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 317/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 318/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 319/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 320/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 321/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 322/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 323/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 324/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 325/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 326/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 327/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 328/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 329/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 330/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 331/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 332/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 333/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 334/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 335/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 336/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 337/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 338/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 339/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 340/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 341/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 342/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 343/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 344/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 345/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 346/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 347/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 348/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 349/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 350/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 351/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 352/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 353/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 354/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 355/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 356/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 357/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 358/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 359/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 360/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 361/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 362/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 363/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 364/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 365/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 366/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 367/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 368/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 369/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 370/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 371/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 372/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 373/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 374/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 375/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 376/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 377/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 378/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 379/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 380/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 381/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 382/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 383/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 384/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 385/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 386/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 387/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 388/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 389/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 390/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 391/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 392/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 393/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 394/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 395/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 396/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 397/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 398/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 399/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 400/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 401/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 402/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 403/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 404/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 405/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 406/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 407/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 408/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 409/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 410/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 411/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 412/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 413/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 414/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 415/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 416/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 417/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 418/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 419/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 420/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 421/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 422/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 423/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 424/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 425/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 426/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 427/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 428/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 429/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 430/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 431/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 432/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 433/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 434/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 435/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 436/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 437/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 438/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 439/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 440/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 441/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 442/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 443/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 444/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 445/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 446/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 447/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 448/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 449/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 450/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 451/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 452/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 453/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 454/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 455/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 456/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 457/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 458/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 459/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 460/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 461/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 462/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 463/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 464/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 465/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 466/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 467/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 468/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 469/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 470/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 471/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 472/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 473/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 474/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 475/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 476/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 477/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 478/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 479/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 480/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 481/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 482/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 483/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 484/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 485/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 486/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 487/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 488/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 489/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 490/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 491/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 492/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 493/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 494/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 495/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 496/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 497/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 498/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 499/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 500/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 501/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 502/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 503/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 504/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 505/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 506/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 507/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 508/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 509/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 510/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 511/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 512/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 513/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 514/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 515/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 516/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 517/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 518/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 519/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 520/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 521/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 522/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 523/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 524/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 525/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 526/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 527/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 528/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 529/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 530/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 531/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 532/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 533/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 534/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 535/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 536/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 537/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 538/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 539/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 540/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 541/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 542/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 543/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 544/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 545/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 546/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 547/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 548/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 549/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 550/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 551/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 552/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 553/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 554/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 555/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 556/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 557/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 558/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 559/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 560/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 561/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 562/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 563/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 564/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 565/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 566/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 567/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 568/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 569/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 570/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 571/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 572/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 573/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 574/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 575/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 576/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 577/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 578/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 579/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 580/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 581/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 582/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 583/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 584/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 585/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 586/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 587/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 588/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 589/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 590/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 591/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 592/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 593/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 594/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 595/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 596/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 597/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 598/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 599/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 600/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 601/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 602/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 603/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 604/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 605/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 606/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 607/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 608/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 609/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 610/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 611/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 612/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 613/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 614/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 615/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 616/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 617/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 618/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 619/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 620/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 621/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 622/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 623/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 624/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 625/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 626/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 627/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 628/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 629/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 630/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 631/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 632/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 633/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 634/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 635/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 636/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 637/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 638/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 639/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 640/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 641/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 642/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 643/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 644/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 645/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 646/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 647/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 648/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 649/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 650/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 651/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 652/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 653/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 654/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 655/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 656/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 657/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 658/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 659/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 660/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 661/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 662/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 663/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 664/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 665/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 666/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 667/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 668/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 669/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 670/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 671/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 672/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 673/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 674/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 675/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 676/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 677/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 678/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 679/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 680/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 681/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 682/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 683/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 684/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 685/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 686/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 687/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 688/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 689/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 690/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 691/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 692/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 693/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 694/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 695/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 696/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 697/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 698/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 699/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 700/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 701/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 702/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 703/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 704/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 705/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 706/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 707/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 708/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 709/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 710/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 711/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 712/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 713/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 714/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 715/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 716/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 717/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 718/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 719/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 720/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 721/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 722/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 723/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 724/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 725/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 726/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 727/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 728/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 729/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 730/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 731/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 732/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 733/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 734/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 735/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 736/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 737/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 738/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 739/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 740/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 741/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 742/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 743/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 744/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 745/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 746/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 747/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 748/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 749/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 750/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 751/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 752/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 753/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 754/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 755/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 756/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 757/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 758/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 759/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 760/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 761/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 762/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 763/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 764/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 765/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 766/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 767/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 768/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 769/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 770/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 771/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 772/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 773/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 774/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 775/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 776/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 777/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 778/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 779/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 780/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 781/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 782/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 783/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 784/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 785/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 786/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 787/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 788/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 789/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 790/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 791/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 792/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 793/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 794/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 795/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 796/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 797/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 798/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 799/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 800/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 801/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 802/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 803/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 804/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 805/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 806/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 807/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 808/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 809/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 810/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 811/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 812/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 813/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 814/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 815/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 816/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 817/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 818/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 819/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 820/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 821/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 822/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 823/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 824/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 825/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 826/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 827/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 828/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 829/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 830/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 831/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 832/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 833/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 834/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 835/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 836/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 837/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 838/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 839/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 840/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 841/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 842/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 843/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 844/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 845/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 846/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 847/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 848/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 849/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 850/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 851/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 852/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 853/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 854/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 855/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 856/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 857/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 858/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 859/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 860/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 861/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 862/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 863/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 864/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 865/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 866/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 867/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 868/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 869/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 870/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 871/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 872/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 873/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 874/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 875/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 876/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 877/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 878/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 879/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 880/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 881/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 882/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 883/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 884/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 885/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 886/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 887/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 888/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 889/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 890/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 891/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 892/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 893/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 894/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 895/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 896/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 897/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 898/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 899/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 900/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 901/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 902/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 903/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 904/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 905/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 906/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 907/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 908/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 909/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 910/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 911/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 912/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 913/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 914/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 915/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 916/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 917/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 918/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 919/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 920/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 921/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 922/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 923/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 924/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 925/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 926/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 927/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 928/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 929/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 930/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 931/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 932/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 933/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 934/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 935/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 936/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 937/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 938/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 939/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 940/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 941/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 942/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 943/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 944/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 945/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 946/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 947/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 948/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 949/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 950/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 951/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 952/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 953/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 954/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 955/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 956/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 957/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 958/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 959/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 960/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 961/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 962/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 963/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 964/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 965/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 966/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 967/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 968/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 969/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 970/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 971/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 972/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 973/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 974/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 975/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 976/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 977/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 978/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 979/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 980/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 981/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 982/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 983/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 984/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 985/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 986/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 987/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 988/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 989/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 990/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 991/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 992/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 993/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 994/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 995/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 996/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 997/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 998/1000 Step: 0 Loss: 0.13532227277755737\n",
      "Epoch: 999/1000 Step: 0 Loss: 0.13532227277755737\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "optim = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "model = model.to(torch.float32)\n",
    "\n",
    "weights = []\n",
    "params = list(model.parameters()).copy()\n",
    "for i in range(len(params)):\n",
    "    weights += params[i].flatten().detach().tolist()\n",
    "weights = np.array(weights)\n",
    "\n",
    "sample = iter(train_loader)\n",
    "x,y = next(sample)\n",
    "x = x.to(torch.float32)\n",
    "y = y.to(torch.float32)\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "# model.l3(model.l2(model.l1(x)))\n",
    "\n",
    "c1,c2 = 0.3,0.3 #cog,soc\n",
    "l = [particle(0.8,c1,c2,weights.shape[0],model,loss,x,y) for i in range(500)]\n",
    "gbest = np.ones(weights.shape[0]+1)*10000\n",
    "calc_gbest(l)\n",
    "\n",
    "def iterate():\n",
    "    calc_gbest(l)\n",
    "    \n",
    "    for i in l:\n",
    "        i.iterate(gbest)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "        j.backward()\n",
    "        # optim.step()\n",
    "\n",
    "        iterate()\n",
    "\n",
    "        losses += [gbest[0].item()]\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs} Step: {i} Loss: {gbest[0].item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.39999999999999\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "\n",
    "    for x,y in test_loader:\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        # d = c + model.l3.out_features\n",
    "\n",
    "        position = gbest[1:]\n",
    "\n",
    "        weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "        bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "        weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "        bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "        model.l1.weight = nn.Parameter(weights1)\n",
    "        model.l1.bias = nn.Parameter(bias1)\n",
    "        model.l3.weight = nn.Parameter(weights2)\n",
    "        model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        pred = torch.round(output).reshape(1,-1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (pred==y).sum().item()\n",
    "        # print(n_correct)\n",
    "    acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "with open(\"./losses/pso.bin\",'wb') as file:\n",
    "    losses.tofile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
