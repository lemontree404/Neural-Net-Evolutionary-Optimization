{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "# df1 = df[df[\"Personal Loan\"]==1]\n",
    "# df0 = df[df[\"Personal Loan\"]==0][:480]\n",
    "# df = pd.concat((df0,df1))\n",
    "# df = df.sample(frac=1)\n",
    "df.drop(\"ID\",axis=1,inplace=True)\n",
    "\n",
    "features = df.drop(\"Personal Loan\",axis=1).columns\n",
    "target = \"Personal Loan\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(df[features],df[target],test_size=0.2)\n",
    "\n",
    "traindb = pd.concat((trainx,trainy),axis=1)\n",
    "testdb = pd.concat((testx,testy),axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "class db(Dataset):\n",
    "    def __init__(self,target,features,df) -> None:\n",
    "        self.y  = torch.tensor(df[target].values)\n",
    "        self.x = torch.tensor(df[features].values)\n",
    "        self.x = scaler.fit_transform(self.x)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "\n",
    "traindbt = db(target,features,traindb)\n",
    "testdbt = db(target,features,testdb)\n",
    "\n",
    "train_loader = DataLoader(traindbt,batch_size=len(traindb),shuffle=True)\n",
    "test_loader = DataLoader(testdbt,batch_size=len(testdb),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(net,self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(12,16)\n",
    "        self.l2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(16,1)\n",
    "        self.l4 = nn.Sigmoid()\n",
    "        # self.l5 = nn.Linear(20,1)\n",
    "        # self.l6 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        x = X\n",
    "\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        # x = self.l5(x)\n",
    "        # x = self.l6(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def flattened_weights(self):\n",
    "        p = []\n",
    "        params = list(self.parameters()).copy()\n",
    "        for i in range(len(params)):\n",
    "            p += params[i].flatten().detach().tolist()\n",
    "        p = np.array(p)\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Step: 0 Loss: 0.4506737291812897\n",
      "Epoch: 1/1000 Step: 0 Loss: 0.45067375898361206\n",
      "Epoch: 2/1000 Step: 0 Loss: 0.4506736993789673\n",
      "Epoch: 3/1000 Step: 0 Loss: 0.4506736695766449\n",
      "Epoch: 4/1000 Step: 0 Loss: 0.4506736993789673\n",
      "Epoch: 5/1000 Step: 0 Loss: 0.4506736993789673\n",
      "Epoch: 6/1000 Step: 0 Loss: 0.4506737291812897\n",
      "Epoch: 7/1000 Step: 0 Loss: 0.4374731779098511\n",
      "Epoch: 8/1000 Step: 0 Loss: 0.4291662275791168\n",
      "Epoch: 9/1000 Step: 0 Loss: 0.4291662275791168\n",
      "Epoch: 10/1000 Step: 0 Loss: 0.3690619170665741\n",
      "Epoch: 11/1000 Step: 0 Loss: 0.3472617268562317\n",
      "Epoch: 12/1000 Step: 0 Loss: 0.3378925025463104\n",
      "Epoch: 13/1000 Step: 0 Loss: 0.31668820977211\n",
      "Epoch: 14/1000 Step: 0 Loss: 0.3166882395744324\n",
      "Epoch: 15/1000 Step: 0 Loss: 0.31668820977211\n",
      "Epoch: 16/1000 Step: 0 Loss: 0.31668820977211\n",
      "Epoch: 17/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 18/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 19/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 20/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 21/1000 Step: 0 Loss: 0.3091922402381897\n",
      "Epoch: 22/1000 Step: 0 Loss: 0.3091922402381897\n",
      "Epoch: 23/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 24/1000 Step: 0 Loss: 0.3091922402381897\n",
      "Epoch: 25/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 26/1000 Step: 0 Loss: 0.3091922402381897\n",
      "Epoch: 27/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 28/1000 Step: 0 Loss: 0.3091922700405121\n",
      "Epoch: 29/1000 Step: 0 Loss: 0.30821874737739563\n",
      "Epoch: 30/1000 Step: 0 Loss: 0.3063059449195862\n",
      "Epoch: 31/1000 Step: 0 Loss: 0.30302390456199646\n",
      "Epoch: 32/1000 Step: 0 Loss: 0.302858829498291\n",
      "Epoch: 33/1000 Step: 0 Loss: 0.3009359836578369\n",
      "Epoch: 34/1000 Step: 0 Loss: 0.3009359836578369\n",
      "Epoch: 35/1000 Step: 0 Loss: 0.3009359836578369\n",
      "Epoch: 36/1000 Step: 0 Loss: 0.3009359836578369\n",
      "Epoch: 37/1000 Step: 0 Loss: 0.3009360134601593\n",
      "Epoch: 38/1000 Step: 0 Loss: 0.3009359836578369\n",
      "Epoch: 39/1000 Step: 0 Loss: 0.30030450224876404\n",
      "Epoch: 40/1000 Step: 0 Loss: 0.30030447244644165\n",
      "Epoch: 41/1000 Step: 0 Loss: 0.3001692593097687\n",
      "Epoch: 42/1000 Step: 0 Loss: 0.3001692295074463\n",
      "Epoch: 43/1000 Step: 0 Loss: 0.2998093366622925\n",
      "Epoch: 44/1000 Step: 0 Loss: 0.2994266450405121\n",
      "Epoch: 45/1000 Step: 0 Loss: 0.2994266450405121\n",
      "Epoch: 46/1000 Step: 0 Loss: 0.2993629276752472\n",
      "Epoch: 47/1000 Step: 0 Loss: 0.29927578568458557\n",
      "Epoch: 48/1000 Step: 0 Loss: 0.29927581548690796\n",
      "Epoch: 49/1000 Step: 0 Loss: 0.29927578568458557\n",
      "Epoch: 50/1000 Step: 0 Loss: 0.29927581548690796\n",
      "Epoch: 51/1000 Step: 0 Loss: 0.29927581548690796\n",
      "Epoch: 52/1000 Step: 0 Loss: 0.29927578568458557\n",
      "Epoch: 53/1000 Step: 0 Loss: 0.2990889847278595\n",
      "Epoch: 54/1000 Step: 0 Loss: 0.2990889549255371\n",
      "Epoch: 55/1000 Step: 0 Loss: 0.29896312952041626\n",
      "Epoch: 56/1000 Step: 0 Loss: 0.29896312952041626\n",
      "Epoch: 57/1000 Step: 0 Loss: 0.2989274263381958\n",
      "Epoch: 58/1000 Step: 0 Loss: 0.2989274263381958\n",
      "Epoch: 59/1000 Step: 0 Loss: 0.2989133894443512\n",
      "Epoch: 60/1000 Step: 0 Loss: 0.2989133596420288\n",
      "Epoch: 61/1000 Step: 0 Loss: 0.2988891303539276\n",
      "Epoch: 62/1000 Step: 0 Loss: 0.2988373339176178\n",
      "Epoch: 63/1000 Step: 0 Loss: 0.29880475997924805\n",
      "Epoch: 64/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 65/1000 Step: 0 Loss: 0.2987229824066162\n",
      "Epoch: 66/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 67/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 68/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 69/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 70/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 71/1000 Step: 0 Loss: 0.2987230122089386\n",
      "Epoch: 72/1000 Step: 0 Loss: 0.2986766993999481\n",
      "Epoch: 73/1000 Step: 0 Loss: 0.2986767292022705\n",
      "Epoch: 74/1000 Step: 0 Loss: 0.2986767292022705\n",
      "Epoch: 75/1000 Step: 0 Loss: 0.2986767292022705\n",
      "Epoch: 76/1000 Step: 0 Loss: 0.29867255687713623\n",
      "Epoch: 77/1000 Step: 0 Loss: 0.298672616481781\n",
      "Epoch: 78/1000 Step: 0 Loss: 0.2986725866794586\n",
      "Epoch: 79/1000 Step: 0 Loss: 0.29866480827331543\n",
      "Epoch: 80/1000 Step: 0 Loss: 0.2986614406108856\n",
      "Epoch: 81/1000 Step: 0 Loss: 0.2986614406108856\n",
      "Epoch: 82/1000 Step: 0 Loss: 0.2986614406108856\n",
      "Epoch: 83/1000 Step: 0 Loss: 0.2986598312854767\n",
      "Epoch: 84/1000 Step: 0 Loss: 0.2986598312854767\n",
      "Epoch: 85/1000 Step: 0 Loss: 0.2986581325531006\n",
      "Epoch: 86/1000 Step: 0 Loss: 0.29865700006484985\n",
      "Epoch: 87/1000 Step: 0 Loss: 0.29865702986717224\n",
      "Epoch: 88/1000 Step: 0 Loss: 0.2986561059951782\n",
      "Epoch: 89/1000 Step: 0 Loss: 0.29865607619285583\n",
      "Epoch: 90/1000 Step: 0 Loss: 0.2986561059951782\n",
      "Epoch: 91/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 92/1000 Step: 0 Loss: 0.29865607619285583\n",
      "Epoch: 93/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 94/1000 Step: 0 Loss: 0.29865607619285583\n",
      "Epoch: 95/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 96/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 97/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 98/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 99/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 100/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 101/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 102/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 103/1000 Step: 0 Loss: 0.29865607619285583\n",
      "Epoch: 104/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 105/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 106/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 107/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 108/1000 Step: 0 Loss: 0.29865607619285583\n",
      "Epoch: 109/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 110/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 111/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 112/1000 Step: 0 Loss: 0.29865607619285583\n",
      "Epoch: 113/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 114/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 115/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 116/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 117/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 118/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 119/1000 Step: 0 Loss: 0.29865604639053345\n",
      "Epoch: 120/1000 Step: 0 Loss: 0.29865601658821106\n",
      "Epoch: 121/1000 Step: 0 Loss: 0.29865580797195435\n",
      "Epoch: 122/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 123/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 124/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 125/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 126/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 127/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 128/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 129/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 130/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 131/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 132/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 133/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 134/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 135/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 136/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 137/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 138/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 139/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 140/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 141/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 142/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 143/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 144/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 145/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 146/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 147/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 148/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 149/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 150/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 151/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 152/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 153/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 154/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 155/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 156/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 157/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 158/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 159/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 160/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 161/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 162/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 163/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 164/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 165/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 166/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 167/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 168/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 169/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 170/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 171/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 172/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 173/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 174/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 175/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 176/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 177/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 178/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 179/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 180/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 181/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 182/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 183/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 184/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 185/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 186/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 187/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 188/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 189/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 190/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 191/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 192/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 193/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 194/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 195/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 196/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 197/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 198/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 199/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 200/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 201/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 202/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 203/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 204/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 205/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 206/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 207/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 208/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 209/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 210/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 211/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 212/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 213/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 214/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 215/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 216/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 217/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 218/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 219/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 220/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 221/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 222/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 223/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 224/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 225/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 226/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 227/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 228/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 229/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 230/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 231/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 232/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 233/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 234/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 235/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 236/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 237/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 238/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 239/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 240/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 241/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 242/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 243/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 244/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 245/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 246/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 247/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 248/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 249/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 250/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 251/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 252/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 253/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 254/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 255/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 256/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 257/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 258/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 259/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 260/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 261/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 262/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 263/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 264/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 265/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 266/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 267/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 268/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 269/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 270/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 271/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 272/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 273/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 274/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 275/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 276/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 277/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 278/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 279/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 280/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 281/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 282/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 283/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 284/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 285/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 286/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 287/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 288/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 289/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 290/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 291/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 292/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 293/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 294/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 295/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 296/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 297/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 298/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 299/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 300/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 301/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 302/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 303/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 304/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 305/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 306/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 307/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 308/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 309/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 310/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 311/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 312/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 313/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 314/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 315/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 316/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 317/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 318/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 319/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 320/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 321/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 322/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 323/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 324/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 325/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 326/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 327/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 328/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 329/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 330/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 331/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 332/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 333/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 334/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 335/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 336/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 337/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 338/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 339/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 340/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 341/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 342/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 343/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 344/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 345/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 346/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 347/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 348/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 349/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 350/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 351/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 352/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 353/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 354/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 355/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 356/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 357/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 358/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 359/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 360/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 361/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 362/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 363/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 364/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 365/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 366/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 367/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 368/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 369/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 370/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 371/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 372/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 373/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 374/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 375/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 376/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 377/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 378/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 379/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 380/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 381/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 382/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 383/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 384/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 385/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 386/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 387/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 388/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 389/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 390/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 391/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 392/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 393/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 394/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 395/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 396/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 397/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 398/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 399/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 400/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 401/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 402/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 403/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 404/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 405/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 406/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 407/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 408/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 409/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 410/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 411/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 412/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 413/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 414/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 415/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 416/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 417/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 418/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 419/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 420/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 421/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 422/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 423/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 424/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 425/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 426/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 427/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 428/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 429/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 430/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 431/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 432/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 433/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 434/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 435/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 436/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 437/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 438/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 439/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 440/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 441/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 442/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 443/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 444/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 445/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 446/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 447/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 448/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 449/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 450/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 451/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 452/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 453/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 454/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 455/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 456/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 457/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 458/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 459/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 460/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 461/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 462/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 463/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 464/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 465/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 466/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 467/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 468/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 469/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 470/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 471/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 472/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 473/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 474/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 475/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 476/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 477/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 478/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 479/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 480/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 481/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 482/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 483/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 484/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 485/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 486/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 487/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 488/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 489/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 490/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 491/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 492/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 493/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 494/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 495/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 496/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 497/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 498/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 499/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 500/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 501/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 502/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 503/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 504/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 505/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 506/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 507/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 508/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 509/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 510/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 511/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 512/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 513/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 514/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 515/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 516/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 517/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 518/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 519/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 520/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 521/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 522/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 523/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 524/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 525/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 526/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 527/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 528/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 529/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 530/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 531/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 532/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 533/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 534/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 535/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 536/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 537/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 538/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 539/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 540/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 541/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 542/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 543/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 544/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 545/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 546/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 547/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 548/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 549/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 550/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 551/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 552/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 553/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 554/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 555/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 556/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 557/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 558/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 559/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 560/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 561/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 562/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 563/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 564/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 565/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 566/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 567/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 568/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 569/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 570/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 571/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 572/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 573/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 574/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 575/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 576/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 577/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 578/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 579/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 580/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 581/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 582/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 583/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 584/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 585/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 586/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 587/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 588/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 589/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 590/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 591/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 592/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 593/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 594/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 595/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 596/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 597/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 598/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 599/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 600/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 601/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 602/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 603/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 604/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 605/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 606/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 607/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 608/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 609/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 610/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 611/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 612/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 613/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 614/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 615/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 616/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 617/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 618/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 619/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 620/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 621/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 622/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 623/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 624/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 625/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 626/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 627/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 628/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 629/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 630/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 631/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 632/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 633/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 634/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 635/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 636/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 637/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 638/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 639/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 640/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 641/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 642/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 643/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 644/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 645/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 646/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 647/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 648/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 649/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 650/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 651/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 652/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 653/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 654/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 655/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 656/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 657/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 658/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 659/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 660/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 661/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 662/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 663/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 664/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 665/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 666/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 667/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 668/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 669/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 670/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 671/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 672/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 673/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 674/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 675/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 676/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 677/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 678/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 679/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 680/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 681/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 682/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 683/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 684/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 685/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 686/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 687/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 688/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 689/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 690/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 691/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 692/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 693/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 694/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 695/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 696/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 697/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 698/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 699/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 700/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 701/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 702/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 703/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 704/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 705/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 706/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 707/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 708/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 709/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 710/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 711/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 712/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 713/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 714/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 715/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 716/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 717/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 718/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 719/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 720/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 721/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 722/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 723/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 724/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 725/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 726/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 727/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 728/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 729/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 730/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 731/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 732/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 733/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 734/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 735/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 736/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 737/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 738/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 739/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 740/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 741/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 742/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 743/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 744/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 745/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 746/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 747/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 748/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 749/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 750/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 751/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 752/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 753/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 754/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 755/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 756/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 757/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 758/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 759/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 760/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 761/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 762/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 763/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 764/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 765/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 766/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 767/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 768/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 769/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 770/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 771/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 772/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 773/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 774/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 775/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 776/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 777/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 778/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 779/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 780/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 781/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 782/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 783/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 784/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 785/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 786/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 787/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 788/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 789/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 790/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 791/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 792/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 793/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 794/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 795/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 796/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 797/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 798/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 799/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 800/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 801/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 802/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 803/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 804/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 805/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 806/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 807/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 808/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 809/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 810/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 811/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 812/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 813/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 814/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 815/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 816/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 817/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 818/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 819/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 820/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 821/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 822/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 823/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 824/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 825/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 826/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 827/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 828/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 829/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 830/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 831/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 832/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 833/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 834/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 835/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 836/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 837/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 838/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 839/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 840/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 841/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 842/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 843/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 844/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 845/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 846/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 847/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 848/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 849/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 850/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 851/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 852/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 853/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 854/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 855/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 856/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 857/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 858/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 859/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 860/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 861/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 862/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 863/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 864/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 865/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 866/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 867/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 868/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 869/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 870/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 871/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 872/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 873/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 874/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 875/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 876/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 877/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 878/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 879/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 880/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 881/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 882/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 883/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 884/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 885/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 886/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 887/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 888/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 889/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 890/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 891/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 892/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 893/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 894/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 895/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 896/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 897/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 898/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 899/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 900/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 901/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 902/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 903/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 904/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 905/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 906/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 907/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 908/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 909/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 910/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 911/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 912/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 913/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 914/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 915/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 916/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 917/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 918/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 919/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 920/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 921/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 922/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 923/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 924/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 925/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 926/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 927/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 928/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 929/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 930/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 931/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 932/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 933/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 934/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 935/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 936/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 937/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 938/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 939/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 940/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 941/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 942/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 943/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 944/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 945/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 946/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 947/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 948/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 949/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 950/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 951/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 952/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 953/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 954/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 955/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 956/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 957/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 958/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 959/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 960/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 961/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 962/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 963/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 964/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 965/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 966/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 967/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 968/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 969/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 970/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 971/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 972/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 973/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 974/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 975/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 976/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 977/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 978/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 979/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 980/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 981/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 982/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 983/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 984/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 985/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 986/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 987/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 988/1000 Step: 0 Loss: 0.29865550994873047\n",
      "Epoch: 989/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 990/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 991/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 992/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 993/1000 Step: 0 Loss: 0.29865556955337524\n",
      "Epoch: 994/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 995/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 996/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 997/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 998/1000 Step: 0 Loss: 0.29865553975105286\n",
      "Epoch: 999/1000 Step: 0 Loss: 0.29865556955337524\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "model = model.to(torch.float32)\n",
    "loss = nn.BCELoss()\n",
    "epochs = 1000\n",
    "\n",
    "n = model.flattened_weights().shape[0] # Problem Space Dimension\n",
    "k = 100 # No. of solutions i.e no. of ants\n",
    "m = 50 # No. of new solutions generated\n",
    "q = 0.6 # Determines the quality of the solutions chosen\n",
    "e = 0.4 # Inversely influences convergence rate\n",
    "T = [np.random.random(n)*2-1 for i in range(k)]\n",
    "\n",
    "def fitness(position,model,loss,x,y):\n",
    "    a = model.l1.in_features * model.l1.out_features\n",
    "    b = a + model.l1.out_features\n",
    "    c = b + (model.l3.in_features * model.l3.out_features)\n",
    "    # d = c + model.l3.out_features\n",
    "\n",
    "    weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "    bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "    weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "    bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "    model.l1.weight = nn.Parameter(weights1)\n",
    "    model.l1.bias = nn.Parameter(bias1)\n",
    "    model.l3.weight = nn.Parameter(weights2)\n",
    "    model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "    y_ = model(x)\n",
    "    j = loss(y_,y.unsqueeze(-1))\n",
    "    \n",
    "    return j\n",
    "\n",
    "sample = iter(train_loader)\n",
    "x,y = next(sample)\n",
    "x = x.to(torch.float32)\n",
    "y = y.to(torch.float32)\n",
    "\n",
    "def iterate():\n",
    "\n",
    "    global T,x,y,model,loss,best,best_ind\n",
    "\n",
    "    fitnesses = [fitness(i,model,loss,x,y).item() for i in T]\n",
    "    ranks = np.argsort(np.argsort(fitnesses))\n",
    "\n",
    "    w = np.array([ np.exp( -(np.square(ranks[j]-1)) / (2 * q**2 * k**2)) / (q * k * np.sqrt(2 * np.pi)) for j in range(k)]) # weights for each solution\n",
    "\n",
    "    p = w/w.sum()\n",
    "\n",
    "    cumsum = np.cumsum(p)\n",
    "\n",
    "    choice = np.random.random()\n",
    "\n",
    "    flag=0\n",
    "    for j in range(len(cumsum)):\n",
    "        if choice<=cumsum[j] and flag==0:\n",
    "            if j!=0:\n",
    "                choice = j-1\n",
    "            else:\n",
    "                choice = j\n",
    "            flag=1\n",
    "\n",
    "    j_ = choice\n",
    "\n",
    "    selected_sol = T[j_]\n",
    "    u_j_ = selected_sol\n",
    "    sd_j_ = np.sum([e * np.abs(T[j_] - T[r]) / (k - 1) for r in range(k)],axis=0)\n",
    "\n",
    "    for i in range(m):\n",
    "        T += [np.random.normal(loc=u_j_,scale=sd_j_,)]\n",
    "\n",
    "    fitnesses = [fitness(i,model,loss,x,y).item() for i in T]\n",
    "\n",
    "    best = np.array(fitnesses).min()\n",
    "\n",
    "    T = [T[i] for i in np.argsort(fitnesses)[:k]]\n",
    "\n",
    "    best_ind = T[0]\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        # optim.zero_grad()\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "        j.backward()\n",
    "        # optim.step()\n",
    "\n",
    "        iterate()\n",
    "\n",
    "        losses += [best]\n",
    "        # population_loss_mean += [pop.pop_mean()]\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs} Step: {i} Loss: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "\n",
    "    for x,y in test_loader:\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        # d = c + model.l3.out_features\n",
    "\n",
    "        position = best_ind\n",
    "\n",
    "        weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "        bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "        weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "        bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "        model.l1.weight = nn.Parameter(weights1)\n",
    "        model.l1.bias = nn.Parameter(bias1)\n",
    "        model.l3.weight = nn.Parameter(weights2)\n",
    "        model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        pred = torch.round(output).reshape(1,-1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (pred==y).sum().item()\n",
    "        # print(n_correct)\n",
    "    acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x271570ef880>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
