{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "# df1 = df[df[\"Personal Loan\"]==1]\n",
    "# df0 = df[df[\"Personal Loan\"]==0][:480]\n",
    "# df = pd.concat((df0,df1))\n",
    "# df = df.sample(frac=1)\n",
    "df.drop(\"ID\",axis=1,inplace=True)\n",
    "\n",
    "features = df.drop(\"Personal Loan\",axis=1).columns\n",
    "target = \"Personal Loan\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(df[features],df[target],test_size=0.2)\n",
    "\n",
    "traindb = pd.concat((trainx,trainy),axis=1)\n",
    "testdb = pd.concat((testx,testy),axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "class db(Dataset):\n",
    "    def __init__(self,target,features,df) -> None:\n",
    "        self.y  = torch.tensor(df[target].values)\n",
    "        self.x = torch.tensor(df[features].values)\n",
    "        self.x = scaler.fit_transform(self.x)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "\n",
    "traindbt = db(target,features,traindb)\n",
    "testdbt = db(target,features,testdb)\n",
    "\n",
    "train_loader = DataLoader(traindbt,batch_size=len(traindb),shuffle=True)\n",
    "test_loader = DataLoader(testdbt,batch_size=len(testdb),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(net,self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(12,16)\n",
    "        self.l2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(16,1)\n",
    "        self.l4 = nn.Sigmoid()\n",
    "        # self.l5 = nn.Linear(20,1)\n",
    "        # self.l6 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        x = X\n",
    "\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        # x = self.l5(x)\n",
    "        # x = self.l6(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def flattened_weights(self):\n",
    "        p = []\n",
    "        params = list(self.parameters()).copy()\n",
    "        for i in range(len(params)):\n",
    "            p += params[i].flatten().detach().tolist()\n",
    "        p = np.array(p)\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Step: 0 Loss: 0.420909583568573\n",
      "Epoch: 1/1000 Step: 0 Loss: 0.4209096133708954\n",
      "Epoch: 2/1000 Step: 0 Loss: 0.41963282227516174\n",
      "Epoch: 3/1000 Step: 0 Loss: 0.363477885723114\n",
      "Epoch: 4/1000 Step: 0 Loss: 0.36347782611846924\n",
      "Epoch: 5/1000 Step: 0 Loss: 0.3634778559207916\n",
      "Epoch: 6/1000 Step: 0 Loss: 0.3634778559207916\n",
      "Epoch: 7/1000 Step: 0 Loss: 0.3634778559207916\n",
      "Epoch: 8/1000 Step: 0 Loss: 0.3634778559207916\n",
      "Epoch: 9/1000 Step: 0 Loss: 0.3634778559207916\n",
      "Epoch: 10/1000 Step: 0 Loss: 0.3548833131790161\n",
      "Epoch: 11/1000 Step: 0 Loss: 0.3548833429813385\n",
      "Epoch: 12/1000 Step: 0 Loss: 0.3431500792503357\n",
      "Epoch: 13/1000 Step: 0 Loss: 0.3431500792503357\n",
      "Epoch: 14/1000 Step: 0 Loss: 0.3151659667491913\n",
      "Epoch: 15/1000 Step: 0 Loss: 0.3151659071445465\n",
      "Epoch: 16/1000 Step: 0 Loss: 0.3110829293727875\n",
      "Epoch: 17/1000 Step: 0 Loss: 0.28435438871383667\n",
      "Epoch: 18/1000 Step: 0 Loss: 0.28435438871383667\n",
      "Epoch: 19/1000 Step: 0 Loss: 0.28435438871383667\n",
      "Epoch: 20/1000 Step: 0 Loss: 0.2843543589115143\n",
      "Epoch: 21/1000 Step: 0 Loss: 0.28435438871383667\n",
      "Epoch: 22/1000 Step: 0 Loss: 0.28435438871383667\n",
      "Epoch: 23/1000 Step: 0 Loss: 0.2843543589115143\n",
      "Epoch: 24/1000 Step: 0 Loss: 0.2809670567512512\n",
      "Epoch: 25/1000 Step: 0 Loss: 0.28096702694892883\n",
      "Epoch: 26/1000 Step: 0 Loss: 0.2770141363143921\n",
      "Epoch: 27/1000 Step: 0 Loss: 0.2770141363143921\n",
      "Epoch: 28/1000 Step: 0 Loss: 0.2647886276245117\n",
      "Epoch: 29/1000 Step: 0 Loss: 0.2647886276245117\n",
      "Epoch: 30/1000 Step: 0 Loss: 0.26478859782218933\n",
      "Epoch: 31/1000 Step: 0 Loss: 0.26478859782218933\n",
      "Epoch: 32/1000 Step: 0 Loss: 0.26011061668395996\n",
      "Epoch: 33/1000 Step: 0 Loss: 0.26011064648628235\n",
      "Epoch: 34/1000 Step: 0 Loss: 0.26011064648628235\n",
      "Epoch: 35/1000 Step: 0 Loss: 0.2595883309841156\n",
      "Epoch: 36/1000 Step: 0 Loss: 0.2595883309841156\n",
      "Epoch: 37/1000 Step: 0 Loss: 0.2595883309841156\n",
      "Epoch: 38/1000 Step: 0 Loss: 0.2595883309841156\n",
      "Epoch: 39/1000 Step: 0 Loss: 0.2585239112377167\n",
      "Epoch: 40/1000 Step: 0 Loss: 0.2585238814353943\n",
      "Epoch: 41/1000 Step: 0 Loss: 0.2585238814353943\n",
      "Epoch: 42/1000 Step: 0 Loss: 0.2585238814353943\n",
      "Epoch: 43/1000 Step: 0 Loss: 0.25834378600120544\n",
      "Epoch: 44/1000 Step: 0 Loss: 0.25834378600120544\n",
      "Epoch: 45/1000 Step: 0 Loss: 0.25834381580352783\n",
      "Epoch: 46/1000 Step: 0 Loss: 0.25798508524894714\n",
      "Epoch: 47/1000 Step: 0 Loss: 0.25798511505126953\n",
      "Epoch: 48/1000 Step: 0 Loss: 0.25779515504837036\n",
      "Epoch: 49/1000 Step: 0 Loss: 0.257795125246048\n",
      "Epoch: 50/1000 Step: 0 Loss: 0.2577846348285675\n",
      "Epoch: 51/1000 Step: 0 Loss: 0.2577846646308899\n",
      "Epoch: 52/1000 Step: 0 Loss: 0.2577846348285675\n",
      "Epoch: 53/1000 Step: 0 Loss: 0.2577846348285675\n",
      "Epoch: 54/1000 Step: 0 Loss: 0.2577846348285675\n",
      "Epoch: 55/1000 Step: 0 Loss: 0.25755977630615234\n",
      "Epoch: 56/1000 Step: 0 Loss: 0.25755977630615234\n",
      "Epoch: 57/1000 Step: 0 Loss: 0.25755977630615234\n",
      "Epoch: 58/1000 Step: 0 Loss: 0.25755977630615234\n",
      "Epoch: 59/1000 Step: 0 Loss: 0.2575056850910187\n",
      "Epoch: 60/1000 Step: 0 Loss: 0.2575056850910187\n",
      "Epoch: 61/1000 Step: 0 Loss: 0.2575056850910187\n",
      "Epoch: 62/1000 Step: 0 Loss: 0.2575056850910187\n",
      "Epoch: 63/1000 Step: 0 Loss: 0.2574915885925293\n",
      "Epoch: 64/1000 Step: 0 Loss: 0.2574915885925293\n",
      "Epoch: 65/1000 Step: 0 Loss: 0.2574833333492279\n",
      "Epoch: 66/1000 Step: 0 Loss: 0.2574627101421356\n",
      "Epoch: 67/1000 Step: 0 Loss: 0.2574619650840759\n",
      "Epoch: 68/1000 Step: 0 Loss: 0.2574521601200104\n",
      "Epoch: 69/1000 Step: 0 Loss: 0.25745218992233276\n",
      "Epoch: 70/1000 Step: 0 Loss: 0.2574465274810791\n",
      "Epoch: 71/1000 Step: 0 Loss: 0.2574384808540344\n",
      "Epoch: 72/1000 Step: 0 Loss: 0.2574384808540344\n",
      "Epoch: 73/1000 Step: 0 Loss: 0.2574279308319092\n",
      "Epoch: 74/1000 Step: 0 Loss: 0.2574279308319092\n",
      "Epoch: 75/1000 Step: 0 Loss: 0.25742796063423157\n",
      "Epoch: 76/1000 Step: 0 Loss: 0.2574279308319092\n",
      "Epoch: 77/1000 Step: 0 Loss: 0.25742796063423157\n",
      "Epoch: 78/1000 Step: 0 Loss: 0.2574247419834137\n",
      "Epoch: 79/1000 Step: 0 Loss: 0.2574220299720764\n",
      "Epoch: 80/1000 Step: 0 Loss: 0.25742125511169434\n",
      "Epoch: 81/1000 Step: 0 Loss: 0.25742125511169434\n",
      "Epoch: 82/1000 Step: 0 Loss: 0.2574188709259033\n",
      "Epoch: 83/1000 Step: 0 Loss: 0.2574188709259033\n",
      "Epoch: 84/1000 Step: 0 Loss: 0.25741884112358093\n",
      "Epoch: 85/1000 Step: 0 Loss: 0.2574188709259033\n",
      "Epoch: 86/1000 Step: 0 Loss: 0.25741884112358093\n",
      "Epoch: 87/1000 Step: 0 Loss: 0.2574188709259033\n",
      "Epoch: 88/1000 Step: 0 Loss: 0.2574184834957123\n",
      "Epoch: 89/1000 Step: 0 Loss: 0.2574184834957123\n",
      "Epoch: 90/1000 Step: 0 Loss: 0.2574184536933899\n",
      "Epoch: 91/1000 Step: 0 Loss: 0.2574184834957123\n",
      "Epoch: 92/1000 Step: 0 Loss: 0.2574184536933899\n",
      "Epoch: 93/1000 Step: 0 Loss: 0.2574184834957123\n",
      "Epoch: 94/1000 Step: 0 Loss: 0.2574184536933899\n",
      "Epoch: 95/1000 Step: 0 Loss: 0.2574184536933899\n",
      "Epoch: 96/1000 Step: 0 Loss: 0.257418155670166\n",
      "Epoch: 97/1000 Step: 0 Loss: 0.257418155670166\n",
      "Epoch: 98/1000 Step: 0 Loss: 0.2574181854724884\n",
      "Epoch: 99/1000 Step: 0 Loss: 0.25741806626319885\n",
      "Epoch: 100/1000 Step: 0 Loss: 0.25741806626319885\n",
      "Epoch: 101/1000 Step: 0 Loss: 0.25741803646087646\n",
      "Epoch: 102/1000 Step: 0 Loss: 0.25741782784461975\n",
      "Epoch: 103/1000 Step: 0 Loss: 0.25741732120513916\n",
      "Epoch: 104/1000 Step: 0 Loss: 0.25741732120513916\n",
      "Epoch: 105/1000 Step: 0 Loss: 0.2574172019958496\n",
      "Epoch: 106/1000 Step: 0 Loss: 0.2574171721935272\n",
      "Epoch: 107/1000 Step: 0 Loss: 0.25741714239120483\n",
      "Epoch: 108/1000 Step: 0 Loss: 0.25741711258888245\n",
      "Epoch: 109/1000 Step: 0 Loss: 0.25741711258888245\n",
      "Epoch: 110/1000 Step: 0 Loss: 0.25741711258888245\n",
      "Epoch: 111/1000 Step: 0 Loss: 0.25741711258888245\n",
      "Epoch: 112/1000 Step: 0 Loss: 0.25741711258888245\n",
      "Epoch: 113/1000 Step: 0 Loss: 0.25741711258888245\n",
      "Epoch: 114/1000 Step: 0 Loss: 0.2574169933795929\n",
      "Epoch: 115/1000 Step: 0 Loss: 0.25741690397262573\n",
      "Epoch: 116/1000 Step: 0 Loss: 0.25741690397262573\n",
      "Epoch: 117/1000 Step: 0 Loss: 0.25741690397262573\n",
      "Epoch: 118/1000 Step: 0 Loss: 0.25741690397262573\n",
      "Epoch: 119/1000 Step: 0 Loss: 0.2574169337749481\n",
      "Epoch: 120/1000 Step: 0 Loss: 0.25741690397262573\n",
      "Epoch: 121/1000 Step: 0 Loss: 0.2574169337749481\n",
      "Epoch: 122/1000 Step: 0 Loss: 0.25741684436798096\n",
      "Epoch: 123/1000 Step: 0 Loss: 0.2574167847633362\n",
      "Epoch: 124/1000 Step: 0 Loss: 0.2574167847633362\n",
      "Epoch: 125/1000 Step: 0 Loss: 0.2574167549610138\n",
      "Epoch: 126/1000 Step: 0 Loss: 0.2574167549610138\n",
      "Epoch: 127/1000 Step: 0 Loss: 0.2574167549610138\n",
      "Epoch: 128/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 129/1000 Step: 0 Loss: 0.2574167549610138\n",
      "Epoch: 130/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 131/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 132/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 133/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 134/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 135/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 136/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 137/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 138/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 139/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 140/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 141/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 142/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 143/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 144/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 145/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 146/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 147/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 148/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 149/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 150/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 151/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 152/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 153/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 154/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 155/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 156/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 157/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 158/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 159/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 160/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 161/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 162/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 163/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 164/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 165/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 166/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 167/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 168/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 169/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 170/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 171/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 172/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 173/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 174/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 175/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 176/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 177/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 178/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 179/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 180/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 181/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 182/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 183/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 184/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 185/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 186/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 187/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 188/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 189/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 190/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 191/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 192/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 193/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 194/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 195/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 196/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 197/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 198/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 199/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 200/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 201/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 202/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 203/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 204/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 205/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 206/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 207/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 208/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 209/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 210/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 211/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 212/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 213/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 214/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 215/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 216/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 217/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 218/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 219/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 220/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 221/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 222/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 223/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 224/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 225/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 226/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 227/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 228/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 229/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 230/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 231/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 232/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 233/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 234/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 235/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 236/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 237/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 238/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 239/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 240/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 241/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 242/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 243/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 244/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 245/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 246/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 247/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 248/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 249/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 250/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 251/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 252/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 253/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 254/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 255/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 256/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 257/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 258/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 259/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 260/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 261/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 262/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 263/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 264/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 265/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 266/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 267/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 268/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 269/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 270/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 271/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 272/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 273/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 274/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 275/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 276/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 277/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 278/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 279/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 280/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 281/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 282/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 283/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 284/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 285/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 286/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 287/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 288/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 289/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 290/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 291/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 292/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 293/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 294/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 295/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 296/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 297/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 298/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 299/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 300/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 301/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 302/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 303/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 304/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 305/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 306/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 307/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 308/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 309/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 310/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 311/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 312/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 313/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 314/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 315/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 316/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 317/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 318/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 319/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 320/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 321/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 322/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 323/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 324/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 325/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 326/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 327/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 328/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 329/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 330/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 331/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 332/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 333/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 334/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 335/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 336/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 337/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 338/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 339/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 340/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 341/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 342/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 343/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 344/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 345/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 346/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 347/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 348/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 349/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 350/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 351/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 352/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 353/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 354/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 355/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 356/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 357/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 358/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 359/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 360/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 361/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 362/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 363/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 364/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 365/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 366/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 367/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 368/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 369/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 370/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 371/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 372/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 373/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 374/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 375/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 376/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 377/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 378/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 379/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 380/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 381/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 382/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 383/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 384/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 385/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 386/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 387/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 388/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 389/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 390/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 391/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 392/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 393/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 394/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 395/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 396/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 397/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 398/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 399/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 400/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 401/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 402/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 403/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 404/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 405/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 406/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 407/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 408/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 409/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 410/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 411/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 412/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 413/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 414/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 415/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 416/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 417/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 418/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 419/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 420/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 421/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 422/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 423/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 424/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 425/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 426/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 427/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 428/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 429/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 430/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 431/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 432/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 433/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 434/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 435/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 436/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 437/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 438/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 439/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 440/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 441/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 442/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 443/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 444/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 445/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 446/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 447/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 448/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 449/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 450/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 451/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 452/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 453/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 454/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 455/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 456/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 457/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 458/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 459/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 460/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 461/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 462/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 463/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 464/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 465/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 466/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 467/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 468/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 469/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 470/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 471/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 472/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 473/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 474/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 475/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 476/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 477/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 478/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 479/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 480/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 481/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 482/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 483/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 484/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 485/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 486/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 487/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 488/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 489/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 490/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 491/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 492/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 493/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 494/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 495/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 496/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 497/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 498/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 499/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 500/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 501/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 502/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 503/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 504/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 505/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 506/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 507/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 508/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 509/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 510/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 511/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 512/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 513/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 514/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 515/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 516/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 517/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 518/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 519/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 520/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 521/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 522/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 523/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 524/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 525/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 526/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 527/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 528/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 529/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 530/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 531/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 532/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 533/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 534/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 535/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 536/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 537/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 538/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 539/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 540/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 541/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 542/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 543/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 544/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 545/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 546/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 547/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 548/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 549/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 550/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 551/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 552/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 553/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 554/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 555/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 556/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 557/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 558/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 559/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 560/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 561/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 562/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 563/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 564/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 565/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 566/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 567/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 568/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 569/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 570/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 571/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 572/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 573/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 574/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 575/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 576/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 577/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 578/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 579/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 580/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 581/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 582/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 583/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 584/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 585/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 586/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 587/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 588/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 589/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 590/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 591/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 592/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 593/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 594/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 595/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 596/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 597/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 598/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 599/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 600/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 601/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 602/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 603/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 604/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 605/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 606/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 607/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 608/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 609/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 610/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 611/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 612/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 613/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 614/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 615/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 616/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 617/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 618/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 619/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 620/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 621/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 622/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 623/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 624/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 625/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 626/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 627/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 628/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 629/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 630/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 631/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 632/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 633/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 634/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 635/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 636/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 637/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 638/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 639/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 640/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 641/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 642/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 643/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 644/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 645/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 646/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 647/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 648/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 649/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 650/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 651/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 652/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 653/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 654/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 655/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 656/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 657/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 658/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 659/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 660/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 661/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 662/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 663/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 664/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 665/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 666/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 667/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 668/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 669/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 670/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 671/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 672/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 673/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 674/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 675/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 676/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 677/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 678/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 679/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 680/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 681/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 682/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 683/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 684/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 685/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 686/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 687/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 688/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 689/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 690/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 691/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 692/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 693/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 694/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 695/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 696/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 697/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 698/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 699/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 700/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 701/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 702/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 703/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 704/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 705/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 706/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 707/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 708/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 709/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 710/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 711/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 712/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 713/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 714/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 715/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 716/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 717/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 718/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 719/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 720/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 721/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 722/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 723/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 724/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 725/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 726/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 727/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 728/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 729/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 730/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 731/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 732/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 733/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 734/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 735/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 736/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 737/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 738/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 739/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 740/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 741/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 742/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 743/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 744/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 745/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 746/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 747/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 748/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 749/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 750/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 751/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 752/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 753/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 754/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 755/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 756/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 757/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 758/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 759/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 760/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 761/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 762/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 763/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 764/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 765/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 766/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 767/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 768/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 769/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 770/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 771/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 772/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 773/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 774/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 775/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 776/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 777/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 778/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 779/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 780/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 781/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 782/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 783/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 784/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 785/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 786/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 787/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 788/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 789/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 790/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 791/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 792/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 793/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 794/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 795/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 796/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 797/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 798/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 799/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 800/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 801/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 802/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 803/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 804/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 805/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 806/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 807/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 808/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 809/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 810/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 811/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 812/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 813/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 814/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 815/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 816/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 817/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 818/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 819/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 820/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 821/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 822/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 823/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 824/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 825/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 826/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 827/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 828/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 829/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 830/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 831/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 832/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 833/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 834/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 835/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 836/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 837/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 838/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 839/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 840/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 841/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 842/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 843/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 844/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 845/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 846/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 847/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 848/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 849/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 850/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 851/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 852/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 853/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 854/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 855/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 856/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 857/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 858/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 859/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 860/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 861/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 862/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 863/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 864/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 865/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 866/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 867/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 868/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 869/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 870/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 871/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 872/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 873/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 874/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 875/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 876/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 877/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 878/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 879/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 880/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 881/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 882/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 883/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 884/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 885/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 886/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 887/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 888/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 889/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 890/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 891/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 892/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 893/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 894/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 895/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 896/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 897/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 898/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 899/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 900/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 901/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 902/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 903/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 904/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 905/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 906/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 907/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 908/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 909/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 910/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 911/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 912/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 913/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 914/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 915/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 916/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 917/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 918/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 919/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 920/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 921/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 922/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 923/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 924/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 925/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 926/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 927/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 928/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 929/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 930/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 931/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 932/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 933/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 934/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 935/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 936/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 937/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 938/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 939/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 940/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 941/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 942/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 943/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 944/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 945/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 946/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 947/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 948/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 949/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 950/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 951/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 952/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 953/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 954/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 955/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 956/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 957/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 958/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 959/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 960/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 961/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 962/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 963/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 964/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 965/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 966/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 967/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 968/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 969/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 970/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 971/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 972/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 973/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 974/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 975/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 976/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 977/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 978/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 979/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 980/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 981/1000 Step: 0 Loss: 0.25741666555404663\n",
      "Epoch: 982/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 983/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 984/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 985/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 986/1000 Step: 0 Loss: 0.2574167251586914\n",
      "Epoch: 987/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 988/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 989/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 990/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 991/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 992/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 993/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 994/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 995/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 996/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 997/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 998/1000 Step: 0 Loss: 0.257416695356369\n",
      "Epoch: 999/1000 Step: 0 Loss: 0.257416695356369\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "model = model.to(torch.float32)\n",
    "loss = nn.BCELoss()\n",
    "epochs = 1000\n",
    "\n",
    "n = model.flattened_weights().shape[0] # Problem Space Dimension\n",
    "k = 100 # No. of solutions i.e no. of ants\n",
    "m = 50 # No. of new solutions generated\n",
    "q = 0.6 # Determines the quality of the solutions chosen\n",
    "e = 0.4 # Inversely influences convergence rate\n",
    "T = [np.random.random(n)*2-1 for i in range(k)]\n",
    "\n",
    "def fitness(position,model,loss,x,y):\n",
    "    a = model.l1.in_features * model.l1.out_features\n",
    "    b = a + model.l1.out_features\n",
    "    c = b + (model.l3.in_features * model.l3.out_features)\n",
    "    # d = c + model.l3.out_features\n",
    "\n",
    "    weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "    bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "    weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "    bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "    model.l1.weight = nn.Parameter(weights1)\n",
    "    model.l1.bias = nn.Parameter(bias1)\n",
    "    model.l3.weight = nn.Parameter(weights2)\n",
    "    model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "    y_ = model(x)\n",
    "    j = loss(y_,y.unsqueeze(-1))\n",
    "    \n",
    "    return j\n",
    "\n",
    "sample = iter(train_loader)\n",
    "x,y = next(sample)\n",
    "x = x.to(torch.float32)\n",
    "y = y.to(torch.float32)\n",
    "\n",
    "def iterate():\n",
    "\n",
    "    global T,x,y,model,loss,best,best_ind\n",
    "\n",
    "    fitnesses = [fitness(i,model,loss,x,y).item() for i in T]\n",
    "    ranks = np.argsort(np.argsort(fitnesses))\n",
    "\n",
    "    w = np.array([ np.exp( -(np.square(ranks[j]-1)) / (2 * q**2 * k**2)) / (q * k * np.sqrt(2 * np.pi)) for j in range(k)]) # weights for each solution\n",
    "\n",
    "    p = w/w.sum()\n",
    "\n",
    "    cumsum = np.cumsum(p)\n",
    "\n",
    "    choice = np.random.random()\n",
    "\n",
    "    flag=0\n",
    "    for j in range(len(cumsum)):\n",
    "        if choice<=cumsum[j] and flag==0:\n",
    "            if j!=0:\n",
    "                choice = j-1\n",
    "            else:\n",
    "                choice = j\n",
    "            flag=1\n",
    "\n",
    "    j_ = choice\n",
    "\n",
    "    selected_sol = T[j_]\n",
    "    u_j_ = selected_sol\n",
    "    sd_j_ = np.sum([e * np.abs(T[j_] - T[r]) / (k - 1) for r in range(k)],axis=0)\n",
    "\n",
    "    for i in range(m):\n",
    "        T += [np.random.normal(loc=u_j_,scale=sd_j_,)]\n",
    "\n",
    "    fitnesses = [fitness(i,model,loss,x,y).item() for i in T]\n",
    "\n",
    "    best = np.array(fitnesses).min()\n",
    "\n",
    "    T = [T[i] for i in np.argsort(fitnesses)[:k]]\n",
    "\n",
    "    best_ind = T[0]\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        # optim.zero_grad()\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "        j.backward()\n",
    "        # optim.step()\n",
    "\n",
    "        iterate()\n",
    "\n",
    "        losses += [best]\n",
    "        # population_loss_mean += [pop.pop_mean()]\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs} Step: {i} Loss: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.8\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "\n",
    "    for x,y in test_loader:\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        # d = c + model.l3.out_features\n",
    "\n",
    "        position = best_ind\n",
    "\n",
    "        weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "        bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "        weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "        bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "        model.l1.weight = nn.Parameter(weights1)\n",
    "        model.l1.bias = nn.Parameter(bias1)\n",
    "        model.l3.weight = nn.Parameter(weights2)\n",
    "        model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        pred = torch.round(output).reshape(1,-1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (pred==y).sum().item()\n",
    "        # print(n_correct)\n",
    "    acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x164db6b8df0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "with open(\"./losses/aco.bin\",'wb') as file:\n",
    "    losses.tofile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
