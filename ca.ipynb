{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data and Loading into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.drop(\"ID\",axis=1,inplace=True)\n",
    "\n",
    "features = df.drop(\"Personal Loan\",axis=1).columns\n",
    "target = \"Personal Loan\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(df[features],df[target],test_size=0.2)\n",
    "\n",
    "traindb = pd.concat((trainx,trainy),axis=1)\n",
    "testdb = pd.concat((testx,testy),axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "class db(Dataset):\n",
    "    def __init__(self,target,features,df) -> None:\n",
    "        self.y  = torch.tensor(df[target].values)\n",
    "        self.x = torch.tensor(df[features].values)\n",
    "        self.x = scaler.fit_transform(self.x)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "\n",
    "traindbt = db(target,features,traindb)\n",
    "testdbt = db(target,features,testdb)\n",
    "\n",
    "train_loader = DataLoader(traindbt,batch_size=len(traindb),shuffle=True)\n",
    "test_loader = DataLoader(testdbt,batch_size=len(testdb),shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network (PyTorch)\n",
    "### Neural Network Architecture:\n",
    "- Input Layer: 12\n",
    "- Hidden Layer: 16\n",
    "- Output Layer: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(net,self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(12,16)\n",
    "        self.l2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(16,1)\n",
    "        self.l4 = nn.Sigmoid()\n",
    "        # self.l5 = nn.Linear(20,1)\n",
    "        # self.l6 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,X):\n",
    "        \n",
    "        x = X\n",
    "\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        # x = self.l5(x)\n",
    "        # x = self.l6(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def flattened_weights(self):\n",
    "        p = []\n",
    "        params = list(self.parameters()).copy()\n",
    "        for i in range(len(params)):\n",
    "            p += params[i].flatten().detach().tolist()\n",
    "        p = np.array(p)\n",
    "\n",
    "        return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Chromosome and Population Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(genes,model,loss,x,y):\n",
    "    a = model.l1.in_features * model.l1.out_features\n",
    "    b = a + model.l1.out_features\n",
    "    c = b + (model.l3.in_features * model.l3.out_features)\n",
    "    # d = c + model.l3.out_features\n",
    "\n",
    "    weights1 = torch.tensor(genes[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "    bias1 = torch.tensor(genes[a:b]).to(torch.float32)\n",
    "    weights2 = torch.tensor(genes[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "    bias2 = torch.tensor(genes[c:]).to(torch.float32)\n",
    "\n",
    "    model.l1.weight = nn.Parameter(weights1)\n",
    "    model.l1.bias = nn.Parameter(bias1)\n",
    "    model.l3.weight = nn.Parameter(weights2)\n",
    "    model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "    y_ = model(x)\n",
    "    j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "    return j.item()\n",
    "    \n",
    "class chromosome:\n",
    "    \n",
    "    def __init__(self,l,model,loss,x,y) -> None:\n",
    "\n",
    "        self.genes = np.random.random(l)*5-2.5\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.fitness = fitness(self.genes,self.model,self.loss,self.x,self.y)\n",
    "    \n",
    "    def calc_fitness(self):\n",
    "        self.fitness = fitness(self.genes,self.model,self.loss,self.x,self.y)\n",
    "\n",
    "    def set_genes(self,genes):\n",
    "        self.genes = genes\n",
    "\n",
    "class population:\n",
    "\n",
    "    def __init__(self,n,model,loss,x,y) -> None:\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.l = model.flattened_weights().shape[0]\n",
    "        self.population = np.array([chromosome(self.l,self.model,self.loss,self.x,self.y) for i in range(50)])\n",
    "    \n",
    "    def pop_mean(self):\n",
    "\n",
    "        return np.array([i.fitness for i in self.population]).mean()\n",
    "\n",
    "    def selection(self) -> None:\n",
    "\n",
    "        fitnesses = np.array([i.fitness for i in self.population])\n",
    "        \n",
    "        choices = list(np.argsort(fitnesses)[:12]) + list(np.argsort(fitnesses)[-4:])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # probs = 1 - fitnesses/fitnesses.sum()\n",
    "        # cumsum = np.cumsum(probs)\n",
    "        # rands = np.random.random(4)\n",
    "        # choices = []\n",
    "        # for i in rands:\n",
    "        #     flag=0\n",
    "        #     for j in range(len(cumsum)):\n",
    "        #         if i<=cumsum[j] and flag==0:\n",
    "        #             if j!=0:\n",
    "        #                 choices+=[j-1]\n",
    "        #             else:\n",
    "        #                 choices+=[j]\n",
    "        #             flag=1\n",
    "        \n",
    "        return [self.population[i] for i in choices]\n",
    "    \n",
    "    def crossover(self,model,pair) -> None:\n",
    "        \n",
    "        parent0 = list(pair[0].genes)\n",
    "        parent1 = list(pair[1].genes)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        d = c + model.l3.out_features\n",
    "\n",
    "        # breakpoints = [0,a//2,a,(b+a)//2,b,(c+b)//2,c,(d+c)//2,d]\n",
    "\n",
    "        breakpoints = [0,a,b,c,d]\n",
    "\n",
    "\n",
    "        offspring0, offspring1 = [], []\n",
    "\n",
    "        for i in range(1,len(breakpoints)-1,2):\n",
    "            offspring0 += parent0[breakpoints[i-1]:breakpoints[i]] + parent1[breakpoints[i]:breakpoints[i+1]]\n",
    "            offspring1 += parent1[breakpoints[i-1]:breakpoints[i]] + parent0[breakpoints[i]:breakpoints[i+1]]\n",
    "\n",
    "        \n",
    "        \n",
    "        a,b = chromosome(self.l,self.model,self.loss,self.x,self.y), chromosome(self.l,self.model,self.loss,self.x,self.y)\n",
    "        a.set_genes(np.array(offspring0))\n",
    "        b.set_genes(np.array(offspring1))\n",
    "\n",
    "        return a,b\n",
    "    \n",
    "    def mutate(self,chrom):\n",
    "\n",
    "        genes = chrom.genes\n",
    "\n",
    "        indices = np.random.randint(0,self.l,(100,2))\n",
    "\n",
    "        for i in indices:\n",
    "            genes[i[0]],genes[i[1]]=genes[i[1]],genes[i[0]]\n",
    "\n",
    "        a = chromosome(self.l,self.model,self.loss,self.x,self.y)\n",
    "        a.set_genes(genes)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def iterate(self):\n",
    "\n",
    "        for i in self.population:\n",
    "            i.calc_fitness()\n",
    "\n",
    "        new_population = self.selection()\n",
    "        offsprings = []\n",
    "        for i in range(0,len(new_population),2):\n",
    "            offsprings += self.crossover(self.model,[new_population[i],new_population[i+1]])\n",
    "            # print(len(offsprings))\n",
    "\n",
    "        new_population += offsprings\n",
    "\n",
    "        # print(len(new_population))\n",
    "\n",
    "        rands = np.random.choice(new_population,18,replace=False).tolist()\n",
    "\n",
    "        for i in rands:\n",
    "            # print(i.genes)\n",
    "            new_population.append(self.mutate(i))\n",
    "            # print(self.mutate(i).genes)\n",
    "        \n",
    "        self.population = new_population\n",
    "\n",
    "        for i in self.population:\n",
    "            i.calc_fitness()\n",
    "        \n",
    "\n",
    "        global best,best_ind\n",
    "        for i in self.population:\n",
    "            if i.fitness<best:\n",
    "                best_ind = i.genes\n",
    "                best = i.fitness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Step: 0 Loss: 2.108177423477173\n",
      "Epoch: 1/1000 Step: 0 Loss: 1.405564308166504\n",
      "Epoch: 2/1000 Step: 0 Loss: 1.405564308166504\n",
      "Epoch: 3/1000 Step: 0 Loss: 1.405564308166504\n",
      "Epoch: 4/1000 Step: 0 Loss: 1.405564308166504\n",
      "Epoch: 5/1000 Step: 0 Loss: 1.405564308166504\n",
      "Epoch: 6/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 7/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 8/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 9/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 10/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 11/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 12/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 13/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 14/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 15/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 16/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 17/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 18/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 19/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 20/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 21/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 22/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 23/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 24/1000 Step: 0 Loss: 1.1525144577026367\n",
      "Epoch: 25/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 26/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 27/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 28/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 29/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 30/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 31/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 32/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 33/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 34/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 35/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 36/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 37/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 38/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 39/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 40/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 41/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 42/1000 Step: 0 Loss: 1.0362515449523926\n",
      "Epoch: 43/1000 Step: 0 Loss: 0.8915999531745911\n",
      "Epoch: 44/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 45/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 46/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 47/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 48/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 49/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 50/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 51/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 52/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 53/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 54/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 55/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 56/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 57/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 58/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 59/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 60/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 61/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 62/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 63/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 64/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 65/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 66/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 67/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 68/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 69/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 70/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 71/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 72/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 73/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 74/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 75/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 76/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 77/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 78/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 79/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 80/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 81/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 82/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 83/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 84/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 85/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 86/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 87/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 88/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 89/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 90/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 91/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 92/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 93/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 94/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 95/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 96/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 97/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 98/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 99/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 100/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 101/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 102/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 103/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 104/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 105/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 106/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 107/1000 Step: 0 Loss: 0.7662476897239685\n",
      "Epoch: 108/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 109/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 110/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 111/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 112/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 113/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 114/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 115/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 116/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 117/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 118/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 119/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 120/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 121/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 122/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 123/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 124/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 125/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 126/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 127/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 128/1000 Step: 0 Loss: 0.7397265434265137\n",
      "Epoch: 129/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 130/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 131/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 132/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 133/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 134/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 135/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 136/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 137/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 138/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 139/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 140/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 141/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 142/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 143/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 144/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 145/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 146/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 147/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 148/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 149/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 150/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 151/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 152/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 153/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 154/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 155/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 156/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 157/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 158/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 159/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 160/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 161/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 162/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 163/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 164/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 165/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 166/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 167/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 168/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 169/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 170/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 171/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 172/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 173/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 174/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 175/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 176/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 177/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 178/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 179/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 180/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 181/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 182/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 183/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 184/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 185/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 186/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 187/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 188/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 189/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 190/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 191/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 192/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 193/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 194/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 195/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 196/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 197/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 198/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 199/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 200/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 201/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 202/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 203/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 204/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 205/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 206/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 207/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 208/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 209/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 210/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 211/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 212/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 213/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 214/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 215/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 216/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 217/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 218/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 219/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 220/1000 Step: 0 Loss: 0.593062698841095\n",
      "Epoch: 221/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 222/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 223/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 224/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 225/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 226/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 227/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 228/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 229/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 230/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 231/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 232/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 233/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 234/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 235/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 236/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 237/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 238/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 239/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 240/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 241/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 242/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 243/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 244/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 245/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 246/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 247/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 248/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 249/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 250/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 251/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 252/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 253/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 254/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 255/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 256/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 257/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 258/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 259/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 260/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 261/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 262/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 263/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 264/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 265/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 266/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 267/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 268/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 269/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 270/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 271/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 272/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 273/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 274/1000 Step: 0 Loss: 0.5149217844009399\n",
      "Epoch: 275/1000 Step: 0 Loss: 0.5053829550743103\n",
      "Epoch: 276/1000 Step: 0 Loss: 0.5053829550743103\n",
      "Epoch: 277/1000 Step: 0 Loss: 0.5022917985916138\n",
      "Epoch: 278/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 279/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 280/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 281/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 282/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 283/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 284/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 285/1000 Step: 0 Loss: 0.49263718724250793\n",
      "Epoch: 286/1000 Step: 0 Loss: 0.48671847581863403\n",
      "Epoch: 287/1000 Step: 0 Loss: 0.48671847581863403\n",
      "Epoch: 288/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 289/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 290/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 291/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 292/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 293/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 294/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 295/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 296/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 297/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 298/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 299/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 300/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 301/1000 Step: 0 Loss: 0.4019811451435089\n",
      "Epoch: 302/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 303/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 304/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 305/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 306/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 307/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 308/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 309/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 310/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 311/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 312/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 313/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 314/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 315/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 316/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 317/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 318/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 319/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 320/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 321/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 322/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 323/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 324/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 325/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 326/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 327/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 328/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 329/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 330/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 331/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 332/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 333/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 334/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 335/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 336/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 337/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 338/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 339/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 340/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 341/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 342/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 343/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 344/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 345/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 346/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 347/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 348/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 349/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 350/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 351/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 352/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 353/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 354/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 355/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 356/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 357/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 358/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 359/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 360/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 361/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 362/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 363/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 364/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 365/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 366/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 367/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 368/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 369/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 370/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 371/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 372/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 373/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 374/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 375/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 376/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 377/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 378/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 379/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 380/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 381/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 382/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 383/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 384/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 385/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 386/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 387/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 388/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 389/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 390/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 391/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 392/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 393/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 394/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 395/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 396/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 397/1000 Step: 0 Loss: 0.3587484061717987\n",
      "Epoch: 398/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 399/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 400/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 401/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 402/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 403/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 404/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 405/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 406/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 407/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 408/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 409/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 410/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 411/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 412/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 413/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 414/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 415/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 416/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 417/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 418/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 419/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 420/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 421/1000 Step: 0 Loss: 0.3583657443523407\n",
      "Epoch: 422/1000 Step: 0 Loss: 0.3538535237312317\n",
      "Epoch: 423/1000 Step: 0 Loss: 0.3538535237312317\n",
      "Epoch: 424/1000 Step: 0 Loss: 0.3538535237312317\n",
      "Epoch: 425/1000 Step: 0 Loss: 0.3538535237312317\n",
      "Epoch: 426/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 427/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 428/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 429/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 430/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 431/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 432/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 433/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 434/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 435/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 436/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 437/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 438/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 439/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 440/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 441/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 442/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 443/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 444/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 445/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 446/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 447/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 448/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 449/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 450/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 451/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 452/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 453/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 454/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 455/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 456/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 457/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 458/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 459/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 460/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 461/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 462/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 463/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 464/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 465/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 466/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 467/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 468/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 469/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 470/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 471/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 472/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 473/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 474/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 475/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 476/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 477/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 478/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 479/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 480/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 481/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 482/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 483/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 484/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 485/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 486/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 487/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 488/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 489/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 490/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 491/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 492/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 493/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 494/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 495/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 496/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 497/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 498/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 499/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 500/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 501/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 502/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 503/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 504/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 505/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 506/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 507/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 508/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 509/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 510/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 511/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 512/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 513/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 514/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 515/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 516/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 517/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 518/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 519/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 520/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 521/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 522/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 523/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 524/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 525/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 526/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 527/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 528/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 529/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 530/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 531/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 532/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 533/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 534/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 535/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 536/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 537/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 538/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 539/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 540/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 541/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 542/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 543/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 544/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 545/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 546/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 547/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 548/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 549/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 550/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 551/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 552/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 553/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 554/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 555/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 556/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 557/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 558/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 559/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 560/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 561/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 562/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 563/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 564/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 565/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 566/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 567/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 568/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 569/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 570/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 571/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 572/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 573/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 574/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 575/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 576/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 577/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 578/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 579/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 580/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 581/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 582/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 583/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 584/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 585/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 586/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 587/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 588/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 589/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 590/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 591/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 592/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 593/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 594/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 595/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 596/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 597/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 598/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 599/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 600/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 601/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 602/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 603/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 604/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 605/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 606/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 607/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 608/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 609/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 610/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 611/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 612/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 613/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 614/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 615/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 616/1000 Step: 0 Loss: 0.2839336693286896\n",
      "Epoch: 617/1000 Step: 0 Loss: 0.2733723223209381\n",
      "Epoch: 618/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 619/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 620/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 621/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 622/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 623/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 624/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 625/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 626/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 627/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 628/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 629/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 630/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 631/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 632/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 633/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 634/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 635/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 636/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 637/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 638/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 639/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 640/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 641/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 642/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 643/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 644/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 645/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 646/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 647/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 648/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 649/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 650/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 651/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 652/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 653/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 654/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 655/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 656/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 657/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 658/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 659/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 660/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 661/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 662/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 663/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 664/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 665/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 666/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 667/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 668/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 669/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 670/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 671/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 672/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 673/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 674/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 675/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 676/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 677/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 678/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 679/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 680/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 681/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 682/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 683/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 684/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 685/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 686/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 687/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 688/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 689/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 690/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 691/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 692/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 693/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 694/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 695/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 696/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 697/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 698/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 699/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 700/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 701/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 702/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 703/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 704/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 705/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 706/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 707/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 708/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 709/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 710/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 711/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 712/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 713/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 714/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 715/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 716/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 717/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 718/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 719/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 720/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 721/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 722/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 723/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 724/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 725/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 726/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 727/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 728/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 729/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 730/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 731/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 732/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 733/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 734/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 735/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 736/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 737/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 738/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 739/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 740/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 741/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 742/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 743/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 744/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 745/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 746/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 747/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 748/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 749/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 750/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 751/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 752/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 753/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 754/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 755/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 756/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 757/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 758/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 759/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 760/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 761/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 762/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 763/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 764/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 765/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 766/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 767/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 768/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 769/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 770/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 771/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 772/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 773/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 774/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 775/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 776/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 777/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 778/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 779/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 780/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 781/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 782/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 783/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 784/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 785/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 786/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 787/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 788/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 789/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 790/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 791/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 792/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 793/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 794/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 795/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 796/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 797/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 798/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 799/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 800/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 801/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 802/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 803/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 804/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 805/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 806/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 807/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 808/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 809/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 810/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 811/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 812/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 813/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 814/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 815/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 816/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 817/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 818/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 819/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 820/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 821/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 822/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 823/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 824/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 825/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 826/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 827/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 828/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 829/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 830/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 831/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 832/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 833/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 834/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 835/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 836/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 837/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 838/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 839/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 840/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 841/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 842/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 843/1000 Step: 0 Loss: 0.26343679428100586\n",
      "Epoch: 844/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 845/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 846/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 847/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 848/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 849/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 850/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 851/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 852/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 853/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 854/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 855/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 856/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 857/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 858/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 859/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 860/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 861/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 862/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 863/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 864/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 865/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 866/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 867/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 868/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 869/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 870/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 871/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 872/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 873/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 874/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 875/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 876/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 877/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 878/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 879/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 880/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 881/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 882/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 883/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 884/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 885/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 886/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 887/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 888/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 889/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 890/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 891/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 892/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 893/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 894/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 895/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 896/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 897/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 898/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 899/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 900/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 901/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 902/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 903/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 904/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 905/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 906/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 907/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 908/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 909/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 910/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 911/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 912/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 913/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 914/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 915/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 916/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 917/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 918/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 919/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 920/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 921/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 922/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 923/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 924/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 925/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 926/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 927/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 928/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 929/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 930/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 931/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 932/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 933/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 934/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 935/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 936/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 937/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 938/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 939/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 940/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 941/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 942/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 943/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 944/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 945/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 946/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 947/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 948/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 949/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 950/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 951/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 952/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 953/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 954/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 955/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 956/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 957/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 958/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 959/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 960/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 961/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 962/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 963/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 964/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 965/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 966/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 967/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 968/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 969/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 970/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 971/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 972/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 973/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 974/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 975/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 976/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 977/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 978/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 979/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 980/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 981/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 982/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 983/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 984/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 985/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 986/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 987/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 988/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 989/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 990/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 991/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 992/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 993/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 994/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 995/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 996/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 997/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 998/1000 Step: 0 Loss: 0.2628689408302307\n",
      "Epoch: 999/1000 Step: 0 Loss: 0.2628689408302307\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "optim = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "model = model.to(torch.float32)\n",
    "\n",
    "sample = iter(train_loader)\n",
    "x,y = next(sample)\n",
    "x = x.to(torch.float32)\n",
    "y = y.to(torch.float32)\n",
    "\n",
    "\n",
    "losses = []\n",
    "population_loss_mean = []\n",
    "\n",
    "best = np.inf\n",
    "best_ind = np.zeros(model.flattened_weights().shape[0])\n",
    "pop = population(10,model,loss,x,y)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "        j.backward()\n",
    "        # optim.step()\n",
    "\n",
    "        pop.iterate()\n",
    "\n",
    "        losses += [best]\n",
    "        population_loss_mean += [pop.pop_mean()]\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs} Step: {i} Loss: {best}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Belief System:\n",
    "- Best individual from the first iteration of training is taken as an oracle.\n",
    "- A belief system is integrated into the fitness function where patterns similar to the oracle are rewarded and patterns that differ from the oracle are penalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = best_ind\n",
    "\n",
    "def belief_integrated_fitness(oracle,genes,model,loss,x,y):\n",
    "    a = model.l1.in_features * model.l1.out_features\n",
    "    b = a + model.l1.out_features\n",
    "    c = b + (model.l3.in_features * model.l3.out_features)\n",
    "    # d = c + model.l3.out_features\n",
    "\n",
    "    weights1 = torch.tensor(genes[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "    bias1 = torch.tensor(genes[a:b]).to(torch.float32)\n",
    "    weights2 = torch.tensor(genes[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "    bias2 = torch.tensor(genes[c:]).to(torch.float32)\n",
    "\n",
    "    model.l1.weight = nn.Parameter(weights1)\n",
    "    model.l1.bias = nn.Parameter(bias1)\n",
    "    model.l3.weight = nn.Parameter(weights2)\n",
    "    model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "    y_ = model(x)\n",
    "    j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "    similarity = np.abs(oracle-genes).sum()\n",
    "\n",
    "    return j.item()*similarity\n",
    "\n",
    "class ca_chromosome:\n",
    "    \n",
    "    def __init__(self,oracle,l,model,loss,x,y) -> None:\n",
    "\n",
    "        self.genes = np.random.normal(oracle)\n",
    "        self.oracle = oracle\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.fitness = belief_integrated_fitness(self.oracle,self.genes,self.model,self.loss,self.x,self.y)\n",
    "    \n",
    "    def calc_fitness(self):\n",
    "        self.fitness = belief_integrated_fitness(self.oracle,self.genes,self.model,self.loss,self.x,self.y)\n",
    "\n",
    "    def set_genes(self,genes):\n",
    "        self.genes = genes\n",
    "\n",
    "class ca_population:\n",
    "\n",
    "    def __init__(self,oracle,n,model,loss,x,y) -> None:\n",
    "        \n",
    "        self.oracle = oracle\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.l = model.flattened_weights().shape[0]\n",
    "        self.population = np.array([ca_chromosome(self.oracle,self.l,self.model,self.loss,self.x,self.y) for i in range(50)])\n",
    "    \n",
    "    def pop_mean(self):\n",
    "\n",
    "        return np.array([i.fitness for i in self.population]).mean()\n",
    "\n",
    "    def selection(self) -> None:\n",
    "\n",
    "        fitnesses = np.array([i.fitness for i in self.population])\n",
    "        \n",
    "        choices = list(np.argsort(fitnesses)[:12]) + list(np.argsort(fitnesses)[-4:])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # probs = 1 - fitnesses/fitnesses.sum()\n",
    "        # cumsum = np.cumsum(probs)\n",
    "        # rands = np.random.random(4)\n",
    "        # choices = []\n",
    "        # for i in rands:\n",
    "        #     flag=0\n",
    "        #     for j in range(len(cumsum)):\n",
    "        #         if i<=cumsum[j] and flag==0:\n",
    "        #             if j!=0:\n",
    "        #                 choices+=[j-1]\n",
    "        #             else:\n",
    "        #                 choices+=[j]\n",
    "        #             flag=1\n",
    "        \n",
    "        return [self.population[i] for i in choices]\n",
    "    \n",
    "    def crossover(self,model,pair) -> None:\n",
    "        \n",
    "        parent0 = list(pair[0].genes)\n",
    "        parent1 = list(pair[1].genes)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        d = c + model.l3.out_features\n",
    "\n",
    "        # breakpoints = [0,a//2,a,(b+a)//2,b,(c+b)//2,c,(d+c)//2,d]\n",
    "\n",
    "        breakpoints = [0,a,b,c,d]\n",
    "\n",
    "\n",
    "        offspring0, offspring1 = [], []\n",
    "\n",
    "        for i in range(1,len(breakpoints)-1,2):\n",
    "            offspring0 += parent0[breakpoints[i-1]:breakpoints[i]] + parent1[breakpoints[i]:breakpoints[i+1]]\n",
    "            offspring1 += parent1[breakpoints[i-1]:breakpoints[i]] + parent0[breakpoints[i]:breakpoints[i+1]]\n",
    "\n",
    "        \n",
    "        \n",
    "        a,b = chromosome(self.l,self.model,self.loss,self.x,self.y), chromosome(self.l,self.model,self.loss,self.x,self.y)\n",
    "        a.set_genes(np.array(offspring0))\n",
    "        b.set_genes(np.array(offspring1))\n",
    "\n",
    "        return a,b\n",
    "    \n",
    "    def mutate(self,chrom):\n",
    "\n",
    "        genes = chrom.genes\n",
    "\n",
    "        indices = np.random.randint(0,self.l,(100,2))\n",
    "\n",
    "        for i in indices:\n",
    "            genes[i[0]],genes[i[1]]=genes[i[1]],genes[i[0]]\n",
    "\n",
    "        a = chromosome(self.l,self.model,self.loss,self.x,self.y)\n",
    "        a.set_genes(genes)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def iterate(self):\n",
    "\n",
    "        for i in self.population:\n",
    "            i.calc_fitness()\n",
    "\n",
    "        new_population = self.selection()\n",
    "        offsprings = []\n",
    "        for i in range(0,len(new_population),2):\n",
    "            offsprings += self.crossover(self.model,[new_population[i],new_population[i+1]])\n",
    "            # print(len(offsprings))\n",
    "\n",
    "        new_population += offsprings\n",
    "\n",
    "        # print(len(new_population))\n",
    "\n",
    "        rands = np.random.choice(new_population,18,replace=False).tolist()\n",
    "\n",
    "        for i in rands:\n",
    "            # print(i.genes)\n",
    "            new_population.append(self.mutate(i))\n",
    "            # print(self.mutate(i).genes)\n",
    "        \n",
    "        self.population = new_population\n",
    "\n",
    "        for i in self.population:\n",
    "            i.calc_fitness()\n",
    "        \n",
    "\n",
    "        global best,best_ind\n",
    "        for i in self.population:\n",
    "            if i.fitness<best:\n",
    "                best_ind = i.genes\n",
    "                best = i.fitness\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Belief Space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000 Step: 0 Loss: 0.9175765514373779\n",
      "Epoch: 1/1000 Step: 0 Loss: 0.8966870903968811\n",
      "Epoch: 2/1000 Step: 0 Loss: 0.6069997549057007\n",
      "Epoch: 3/1000 Step: 0 Loss: 0.6069997549057007\n",
      "Epoch: 4/1000 Step: 0 Loss: 0.6069997549057007\n",
      "Epoch: 5/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 6/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 7/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 8/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 9/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 10/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 11/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 12/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 13/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 14/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 15/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 16/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 17/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 18/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 19/1000 Step: 0 Loss: 0.49997714161872864\n",
      "Epoch: 20/1000 Step: 0 Loss: 0.42978182435035706\n",
      "Epoch: 21/1000 Step: 0 Loss: 0.42978182435035706\n",
      "Epoch: 22/1000 Step: 0 Loss: 0.42978182435035706\n",
      "Epoch: 23/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 24/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 25/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 26/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 27/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 28/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 29/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 30/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 31/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 32/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 33/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 34/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 35/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 36/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 37/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 38/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 39/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 40/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 41/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 42/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 43/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 44/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 45/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 46/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 47/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 48/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 49/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 50/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 51/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 52/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 53/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 54/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 55/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 56/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 57/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 58/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 59/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 60/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 61/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 62/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 63/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 64/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 65/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 66/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 67/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 68/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 69/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 70/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 71/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 72/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 73/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 74/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 75/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 76/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 77/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 78/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 79/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 80/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 81/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 82/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 83/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 84/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 85/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 86/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 87/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 88/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 89/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 90/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 91/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 92/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 93/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 94/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 95/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 96/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 97/1000 Step: 0 Loss: 0.3633876144886017\n",
      "Epoch: 98/1000 Step: 0 Loss: 0.34352973103523254\n",
      "Epoch: 99/1000 Step: 0 Loss: 0.34352973103523254\n",
      "Epoch: 100/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 101/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 102/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 103/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 104/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 105/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 106/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 107/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 108/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 109/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 110/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 111/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 112/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 113/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 114/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 115/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 116/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 117/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 118/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 119/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 120/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 121/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 122/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 123/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 124/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 125/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 126/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 127/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 128/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 129/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 130/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 131/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 132/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 133/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 134/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 135/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 136/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 137/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 138/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 139/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 140/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 141/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 142/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 143/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 144/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 145/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 146/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 147/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 148/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 149/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 150/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 151/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 152/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 153/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 154/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 155/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 156/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 157/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 158/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 159/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 160/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 161/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 162/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 163/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 164/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 165/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 166/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 167/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 168/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 169/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 170/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 171/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 172/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 173/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 174/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 175/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 176/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 177/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 178/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 179/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 180/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 181/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 182/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 183/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 184/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 185/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 186/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 187/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 188/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 189/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 190/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 191/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 192/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 193/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 194/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 195/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 196/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 197/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 198/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 199/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 200/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 201/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 202/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 203/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 204/1000 Step: 0 Loss: 0.3174682855606079\n",
      "Epoch: 205/1000 Step: 0 Loss: 0.3117400109767914\n",
      "Epoch: 206/1000 Step: 0 Loss: 0.3117400109767914\n",
      "Epoch: 207/1000 Step: 0 Loss: 0.3117400109767914\n",
      "Epoch: 208/1000 Step: 0 Loss: 0.3117400109767914\n",
      "Epoch: 209/1000 Step: 0 Loss: 0.3117400109767914\n",
      "Epoch: 210/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 211/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 212/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 213/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 214/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 215/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 216/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 217/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 218/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 219/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 220/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 221/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 222/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 223/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 224/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 225/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 226/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 227/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 228/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 229/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 230/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 231/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 232/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 233/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 234/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 235/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 236/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 237/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 238/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 239/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 240/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 241/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 242/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 243/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 244/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 245/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 246/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 247/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 248/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 249/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 250/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 251/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 252/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 253/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 254/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 255/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 256/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 257/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 258/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 259/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 260/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 261/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 262/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 263/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 264/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 265/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 266/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 267/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 268/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 269/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 270/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 271/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 272/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 273/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 274/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 275/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 276/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 277/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 278/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 279/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 280/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 281/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 282/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 283/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 284/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 285/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 286/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 287/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 288/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 289/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 290/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 291/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 292/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 293/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 294/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 295/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 296/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 297/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 298/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 299/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 300/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 301/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 302/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 303/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 304/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 305/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 306/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 307/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 308/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 309/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 310/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 311/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 312/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 313/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 314/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 315/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 316/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 317/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 318/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 319/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 320/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 321/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 322/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 323/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 324/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 325/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 326/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 327/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 328/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 329/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 330/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 331/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 332/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 333/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 334/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 335/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 336/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 337/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 338/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 339/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 340/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 341/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 342/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 343/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 344/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 345/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 346/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 347/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 348/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 349/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 350/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 351/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 352/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 353/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 354/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 355/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 356/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 357/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 358/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 359/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 360/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 361/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 362/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 363/1000 Step: 0 Loss: 0.3036135733127594\n",
      "Epoch: 364/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 365/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 366/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 367/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 368/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 369/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 370/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 371/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 372/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 373/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 374/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 375/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 376/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 377/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 378/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 379/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 380/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 381/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 382/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 383/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 384/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 385/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 386/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 387/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 388/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 389/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 390/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 391/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 392/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 393/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 394/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 395/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 396/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 397/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 398/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 399/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 400/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 401/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 402/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 403/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 404/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 405/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 406/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 407/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 408/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 409/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 410/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 411/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 412/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 413/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 414/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 415/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 416/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 417/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 418/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 419/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 420/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 421/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 422/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 423/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 424/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 425/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 426/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 427/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 428/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 429/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 430/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 431/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 432/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 433/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 434/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 435/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 436/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 437/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 438/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 439/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 440/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 441/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 442/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 443/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 444/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 445/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 446/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 447/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 448/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 449/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 450/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 451/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 452/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 453/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 454/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 455/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 456/1000 Step: 0 Loss: 0.29663872718811035\n",
      "Epoch: 457/1000 Step: 0 Loss: 0.29327213764190674\n",
      "Epoch: 458/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 459/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 460/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 461/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 462/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 463/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 464/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 465/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 466/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 467/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 468/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 469/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 470/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 471/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 472/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 473/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 474/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 475/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 476/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 477/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 478/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 479/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 480/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 481/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 482/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 483/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 484/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 485/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 486/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 487/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 488/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 489/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 490/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 491/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 492/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 493/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 494/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 495/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 496/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 497/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 498/1000 Step: 0 Loss: 0.2877703607082367\n",
      "Epoch: 499/1000 Step: 0 Loss: 0.2868766486644745\n",
      "Epoch: 500/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 501/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 502/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 503/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 504/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 505/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 506/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 507/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 508/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 509/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 510/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 511/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 512/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 513/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 514/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 515/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 516/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 517/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 518/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 519/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 520/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 521/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 522/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 523/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 524/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 525/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 526/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 527/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 528/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 529/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 530/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 531/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 532/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 533/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 534/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 535/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 536/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 537/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 538/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 539/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 540/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 541/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 542/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 543/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 544/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 545/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 546/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 547/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 548/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 549/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 550/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 551/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 552/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 553/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 554/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 555/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 556/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 557/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 558/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 559/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 560/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 561/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 562/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 563/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 564/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 565/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 566/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 567/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 568/1000 Step: 0 Loss: 0.2698238492012024\n",
      "Epoch: 569/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 570/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 571/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 572/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 573/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 574/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 575/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 576/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 577/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 578/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 579/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 580/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 581/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 582/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 583/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 584/1000 Step: 0 Loss: 0.26495012640953064\n",
      "Epoch: 585/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 586/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 587/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 588/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 589/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 590/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 591/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 592/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 593/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 594/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 595/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 596/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 597/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 598/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 599/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 600/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 601/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 602/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 603/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 604/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 605/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 606/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 607/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 608/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 609/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 610/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 611/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 612/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 613/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 614/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 615/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 616/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 617/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 618/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 619/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 620/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 621/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 622/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 623/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 624/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 625/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 626/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 627/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 628/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 629/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 630/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 631/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 632/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 633/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 634/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 635/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 636/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 637/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 638/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 639/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 640/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 641/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 642/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 643/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 644/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 645/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 646/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 647/1000 Step: 0 Loss: 0.2587161958217621\n",
      "Epoch: 648/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 649/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 650/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 651/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 652/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 653/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 654/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 655/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 656/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 657/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 658/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 659/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 660/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 661/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 662/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 663/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 664/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 665/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 666/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 667/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 668/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 669/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 670/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 671/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 672/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 673/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 674/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 675/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 676/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 677/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 678/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 679/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 680/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 681/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 682/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 683/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 684/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 685/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 686/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 687/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 688/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 689/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 690/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 691/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 692/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 693/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 694/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 695/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 696/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 697/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 698/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 699/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 700/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 701/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 702/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 703/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 704/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 705/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 706/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 707/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 708/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 709/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 710/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 711/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 712/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 713/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 714/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 715/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 716/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 717/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 718/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 719/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 720/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 721/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 722/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 723/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 724/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 725/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 726/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 727/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 728/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 729/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 730/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 731/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 732/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 733/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 734/1000 Step: 0 Loss: 0.2538754343986511\n",
      "Epoch: 735/1000 Step: 0 Loss: 0.22310863435268402\n",
      "Epoch: 736/1000 Step: 0 Loss: 0.22310863435268402\n",
      "Epoch: 737/1000 Step: 0 Loss: 0.20982666313648224\n",
      "Epoch: 738/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 739/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 740/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 741/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 742/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 743/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 744/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 745/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 746/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 747/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 748/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 749/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 750/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 751/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 752/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 753/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 754/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 755/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 756/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 757/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 758/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 759/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 760/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 761/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 762/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 763/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 764/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 765/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 766/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 767/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 768/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 769/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 770/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 771/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 772/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 773/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 774/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 775/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 776/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 777/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 778/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 779/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 780/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 781/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 782/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 783/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 784/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 785/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 786/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 787/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 788/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 789/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 790/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 791/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 792/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 793/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 794/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 795/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 796/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 797/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 798/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 799/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 800/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 801/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 802/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 803/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 804/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 805/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 806/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 807/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 808/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 809/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 810/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 811/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 812/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 813/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 814/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 815/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 816/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 817/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 818/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 819/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 820/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 821/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 822/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 823/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 824/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 825/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 826/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 827/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 828/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 829/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 830/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 831/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 832/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 833/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 834/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 835/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 836/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 837/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 838/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 839/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 840/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 841/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 842/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 843/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 844/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 845/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 846/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 847/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 848/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 849/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 850/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 851/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 852/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 853/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 854/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 855/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 856/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 857/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 858/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 859/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 860/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 861/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 862/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 863/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 864/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 865/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 866/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 867/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 868/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 869/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 870/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 871/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 872/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 873/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 874/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 875/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 876/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 877/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 878/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 879/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 880/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 881/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 882/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 883/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 884/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 885/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 886/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 887/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 888/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 889/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 890/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 891/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 892/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 893/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 894/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 895/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 896/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 897/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 898/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 899/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 900/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 901/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 902/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 903/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 904/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 905/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 906/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 907/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 908/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 909/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 910/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 911/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 912/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 913/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 914/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 915/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 916/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 917/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 918/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 919/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 920/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 921/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 922/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 923/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 924/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 925/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 926/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 927/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 928/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 929/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 930/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 931/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 932/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 933/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 934/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 935/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 936/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 937/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 938/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 939/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 940/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 941/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 942/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 943/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 944/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 945/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 946/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 947/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 948/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 949/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 950/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 951/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 952/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 953/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 954/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 955/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 956/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 957/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 958/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 959/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 960/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 961/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 962/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 963/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 964/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 965/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 966/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 967/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 968/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 969/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 970/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 971/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 972/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 973/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 974/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 975/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 976/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 977/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 978/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 979/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 980/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 981/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 982/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 983/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 984/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 985/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 986/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 987/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 988/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 989/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 990/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 991/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 992/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 993/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 994/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 995/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 996/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 997/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 998/1000 Step: 0 Loss: 0.2074355036020279\n",
      "Epoch: 999/1000 Step: 0 Loss: 0.2074355036020279\n"
     ]
    }
   ],
   "source": [
    "sample = iter(train_loader)\n",
    "x,y = next(sample)\n",
    "x = x.to(torch.float32)\n",
    "y = y.to(torch.float32)\n",
    "\n",
    "\n",
    "losses = []\n",
    "population_loss_mean = []\n",
    "\n",
    "best = np.inf\n",
    "best_ind = np.zeros(model.flattened_weights().shape[0])\n",
    "pop = ca_population(oracle,10,model,loss,x,y)\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "        j.backward()\n",
    "        # optim.step()\n",
    "\n",
    "        pop.iterate()\n",
    "\n",
    "        losses += [best]\n",
    "        population_loss_mean += [pop.pop_mean()]\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs} Step: {i} Loss: {best}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.8\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "\n",
    "    for x,y in test_loader:\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "\n",
    "        a = model.l1.in_features * model.l1.out_features\n",
    "        b = a + model.l1.out_features\n",
    "        c = b + (model.l3.in_features * model.l3.out_features)\n",
    "        # d = c + model.l3.out_features\n",
    "\n",
    "        position = best_ind\n",
    "\n",
    "        weights1 = torch.tensor(position[:a]).to(torch.float32).reshape((model.l1.out_features,model.l1.in_features))\n",
    "        bias1 = torch.tensor(position[a:b]).to(torch.float32)\n",
    "        weights2 = torch.tensor(position[b:c]).to(torch.float32).reshape((model.l3.out_features,model.l3.in_features))\n",
    "        bias2 = torch.tensor(position[c:]).to(torch.float32)\n",
    "\n",
    "        model.l1.weight = nn.Parameter(weights1)\n",
    "        model.l1.bias = nn.Parameter(bias1)\n",
    "        model.l3.weight = nn.Parameter(weights2)\n",
    "        model.l3.bias = nn.Parameter(bias2)\n",
    "\n",
    "        y_ = model(x)\n",
    "        j = loss(y_,y.unsqueeze(-1))\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        pred = torch.round(output).reshape(1,-1)\n",
    "        n_samples += y.shape[0]\n",
    "        n_correct += (pred==y).sum().item()\n",
    "        # print(n_correct)\n",
    "    acc = 100*(n_correct/n_samples)\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x234ba8d8be0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "# plt.plot(population_loss_mean,',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "with open(\"./losses/ca.bin\",'wb') as file:\n",
    "    losses.tofile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
